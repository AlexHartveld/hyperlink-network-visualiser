{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d08c57a",
   "metadata": {},
   "source": [
    "# School website scraper in four parts: \n",
    "- Part 1: Set-Up\n",
    "    - set up python environment and jupyter notebook \n",
    "- Part 2-3: Extracting\n",
    "    - read csv input file with school addresses\n",
    "    - visit each school homepage and extract external links,\n",
    "        - miner config for second loop on all internal links\n",
    "    - storing data in intermediate csv files\n",
    "- Part 4-5: Transforming \n",
    "    - link extraction\n",
    "    - link standardization\n",
    "    - categorization\n",
    "- Part 6: Loading of Visualizations\n",
    "    - adding nodes and relations\n",
    "    - modifying nodes and relations\n",
    "\n",
    "### Functional requirements:\n",
    "- parsing an input file\n",
    "- mining school websites for links\n",
    "    - visit 1-3 layers of internal links, look for external links\n",
    "    - store all links in a csv file \n",
    "- visualize it as a graph with colors, by using [pyvis](https://pyvis.readthedocs.io/en/latest/documentation.html?highlight=template#pyvis.network.Network.set_template).\n",
    "- export it as a .html file for webhosting / analysis\n",
    "- using intermediate files, based on date of access and key factors\n",
    "\n",
    "### non-functional requirements:\n",
    "- Easy to set up and follow documentation\n",
    "- Allows for customizability\n",
    "\n",
    "### Status\n",
    "This notebook would need another round of refinement, and some recommended changes are given in the sections. \n",
    "Some duplicate or unused code is still available in the categorization section because this was the most difficult challenge.\n",
    "The quality of this notebook is a proof of concept and does not fulfil my standards of good code. The code can be run and improved in case it is necessary. \n",
    "\n",
    "## Part 1: Set-up\n",
    "- from terminal, set up new environment: \n",
    "```python3 -m venv schoolscraperenv ``` \n",
    "- activate environment: \n",
    "```source schoolscraperenv/bin/activate ``` \n",
    "- install kernel that can be used by jupyter: \n",
    "```pip install ipykernel ``` \n",
    "- activate the created environment as a kernel: \n",
    "```python -m ipykernel install --user --name=schoolscraperenv --display-name \"Python (schoolscraperenv)\" ``` \n",
    "- now it should be listed as an active kernel: \n",
    "```jupyter kernelspec list ``` \n",
    "- install required packages: \n",
    "```python3 -m pip install -r requirements.txt ``` \n",
    "- launch jupyter: \n",
    "```jupyter notebook ``` \n",
    "- now when you open the notebook you should be able to select the new kernel\n",
    "\n",
    "The next cell is complecting the set-up by importing and making available the libraries for the rest of the notebook. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0abb623",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import time \n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin, urlparse, urlsplit, urlunsplit\n",
    "import csv\n",
    "import pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e50e6f3",
   "metadata": {},
   "source": [
    "## Part 2: Parse list of schools\n",
    "Takes a csv with schools as input and cleans up the variables\n",
    "### Part 2.1: Load schools csv file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16cb6e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Part 2.1: Load schools csv file. \n",
    "## Creates df\n",
    "# Tries to make a list of all external links from homepages in a csv\n",
    "import csv\n",
    "import pandas as pd\n",
    "from mining_services import get_netloc\n",
    "\n",
    "csv_path = './public-schooldata/vienna-schools.csv'\n",
    "csv_path_austria = './public-schooldata/austria-schools.csv'\n",
    "\n",
    "\n",
    "# Using Panda: Loading the .csv file into a DataFrame\n",
    "df = pd.read_csv(csv_path)\n",
    "df_austria = pd.read_csv(csv_path_austria)\n",
    "# Inspect the DataFrame\n",
    "print(\"DataFrame Head:\")\n",
    "# print(df.head())\n",
    "\n",
    "# Select relevant columns\n",
    "df = df.iloc[:, [2, 5, 6, 9, 3, 7]]\n",
    "df.columns = ['name', 'school_type_code', 'school_type_txt', 'website_url', 'address', 'SKZ']\n",
    "\n",
    "print(df.head())\n",
    "\n",
    "# Handle missing values\n",
    "print(\"\\nMissing Values:\")\n",
    "print(df.isna().sum())\n",
    "\n",
    "df['school_type_code'] = df['school_type_code'].fillna('Unknown')\n",
    "df['school_type_txt'] = df['school_type_txt'].fillna('Unknown')\n",
    "df['website_url'] = df['website_url'].fillna('No URL')\n",
    "\n",
    "print(\"\\nMissing Values After:\")\n",
    "print(df.isna().sum())\n",
    "\n",
    "# make sure all URL's have a protocol:\n",
    "def normalize_school_url(url):\n",
    "    if url == 'No URL':\n",
    "        return url \n",
    "    elif url.startswith(('http://', 'https://')):\n",
    "        return url  # URL already starts with http:// or https://\n",
    "    else:\n",
    "        print(f\"added http:// to {url}\")\n",
    "        return 'http://' + url  # prepend http:// to the URL\n",
    "df['website_url'] = df['website_url'].apply(normalize_school_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "557756b3",
   "metadata": {},
   "source": [
    "### Part 2.3: Descriptive Statistics\n",
    "Regarding school types, Austria vs. Vienna, etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db493a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 2.3: Descriptive Statistics\n",
    "## Schooltypes Austria vs. Vienna\n",
    "### Schooltypes counts\n",
    "print(\"Austria Categories:\")\n",
    "print(df_austria['TYP'].value_counts())\n",
    "print(\"\\n\\nVienna Categories:\")\n",
    "print(df['school_type_txt'].value_counts())\n",
    "\n",
    "### Schooltypes counts, aggregated\n",
    "\n",
    "\n",
    "#### mapping table\n",
    "aggregate_mapping_vienna = {\n",
    "    'Volksschule (öffentlich)': 'Volksschule',\n",
    "    'Volksschule (privat)': 'Volksschule',\n",
    "    'Mittelschule (öffentlich)': 'Mittelschule',\n",
    "    'Mittelschule (privat)': 'Mittelschule',\n",
    "    'Statutschule (privat)': 'Statutschule',\n",
    "    'Allgemeinbildende höhere Schule (öffentlich)': 'Allgemeinbildende Höhere Schule',\n",
    "    'Allgemeinbildende höhere Schule (privat)': 'Allgemeinbildende Höhere Schule',\n",
    "    'Sonderschule (öffentlich)': 'Sonderschule',\n",
    "    'Sonderschule (privat)': 'Sonderschule',\n",
    "    'Berufsschule (öffentlich)': 'Berufsschule',\n",
    "    'Berufsschule (privat)': 'Berufsschule',\n",
    "    'Humanberufliche Schule (öffentlich)': 'Berufsschule',\n",
    "    'Humanberufliche Schule (privat)': 'Berufsschule',\n",
    "    'Kaufmännische Schule (öffentlich)': 'Berufsbildende Mittlere und Höhere Schule',\n",
    "    'Kaufmännische Schule (privat)': 'Berufsbildende Mittlere und Höhere Schule',\n",
    "    'LehrerInnenbildende mittlere und höhere Schulen (öffentlich)': 'LehrerInnen- und ErzieherInnenbildung',\n",
    "    'LehrerInnenbildende mittlere und höhere Schulen (privat)': 'LehrerInnen- und ErzieherInnenbildung',\n",
    "    'Polytechnische Schule (öffentlich)': 'Polytechnische Schule',\n",
    "    'Polytechnische Schule (privat)': 'Polytechnische Schule',\n",
    "    'Technische Schule (öffentlich)': 'Berufsbildende Mittlere und Höhere Schule',\n",
    "    'Technische Schule (privat)': 'Berufsbildende Mittlere und Höhere Schule',\n",
    "    'Zentrallehranstalt (öffentlich)': 'Berufsbildende Mittlere und Höhere Schule',\n",
    "    'Statutschule (öffentlich)': 'Statutschule',\n",
    "    'Sonstige (öffentlich)': 'Other',\n",
    "    'Sonstige (privat)': 'Other',\n",
    "    'Bundesinstitut (öffentlich)': 'Other'\n",
    "}\n",
    "aggregate_mapping_austria = {\n",
    "    'VS': 'Volksschule',\n",
    "    'NMSH': 'Mittelschule',\n",
    "    'AHS': 'Allgemeinbildende Höhere Schule',\n",
    "    'SS': 'Sonderschule',\n",
    "    'PS': 'Polytechnische Schule',\n",
    "    'BMHST': 'Berufsbildende Mittlere und Höhere Schule',\n",
    "    'ASTAT': 'Statutschule',\n",
    "    'BS': 'Berufsschule',\n",
    "    'GKS': 'Berufsschule',\n",
    "    'BMHSK': 'Berufsbildende Mittlere und Höhere Schule',\n",
    "    'BMHSW': 'Berufsbildende Mittlere und Höhere Schule',\n",
    "    'LFMS': 'Mittelschule',\n",
    "    'BSTAT': 'Statutschule',\n",
    "    'BMHSP': 'LehrerInnen- und ErzieherInnenbildung',\n",
    "    'LFHS': 'Berufsbildende Mittlere und Höhere Schule',\n",
    "    'NMSA': 'Allgemeinbildende Höhere Schule',\n",
    "    'LMS': 'Other',\n",
    "    'BMHSS': 'Berufsschule'\n",
    "}\n",
    "#### some preliminary checks\n",
    "\n",
    "\n",
    "\n",
    "# Perform the mapping\n",
    "#print(aggregate_mapping_austria)\n",
    "df['school_type_mapping'] = df['school_type_txt'].map(aggregate_mapping_vienna)\n",
    "df_austria['school_type_mapping'] = df_austria['TYP'].map(aggregate_mapping_austria)\n",
    "print(df['school_type_mapping'].value_counts())\n",
    "print(df_austria['school_type_mapping'].value_counts())\n",
    "\n",
    "# check if there are discrepancies\n",
    "# Merge the two DataFrames on the 'SKZ' column\n",
    "df_austria['SKZ'] = df_austria['SKZ'].astype(str)\n",
    "df['SKZ'] = df['SKZ'].astype(str)\n",
    "merged_df = pd.merge(df_austria, df, on='SKZ', suffixes=('_dfAT', '_dfVIE'))\n",
    "\n",
    "# Check for discrepancies in the 'school_type_mapping' column\n",
    "discrepancies = merged_df[merged_df['school_type_mapping_dfAT'] != merged_df['school_type_mapping_dfVIE']]\n",
    "\n",
    "print(discrepancies.head)\n",
    "# Print the discrepancies\n",
    "print(f\"\\n\\n{discrepancies.shape[0]} Discrepancies found:\")\n",
    "print(discrepancies[['SKZ', 'name', 'school_type_mapping_dfAT', 'school_type_mapping_dfVIE']])\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719f3581",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "csv_path_austria = './public-schooldata/austria-schools.csv'\n",
    "df_austria = pd.read_csv(csv_path_austria)\n",
    "\n",
    "# Vienna schools have SKZ starting with '9'\n",
    "df_vienna = df_austria[df_austria['SKZ'].astype(str).str.startswith('9')]\n",
    "\n",
    "# Schools from the rest of Austria (SKZ starting with 1-8)\n",
    "df_austria_rest = df_austria[~df_austria['SKZ'].astype(str).str.startswith('9')]\n",
    "\n",
    "# Define a function to calculate these statistics\n",
    "def calculate_school_stats(df, total_schools):\n",
    "    grouped = df.groupby('TYP')\n",
    "    \n",
    "    stats = grouped.size().reset_index(name='Count')\n",
    "    stats['% Private'] = grouped.apply(lambda x: (x['ERHALTER'] == 'privat').sum() / len(x) * 100).values\n",
    "    stats['% of Total'] = stats['Count'] / total_schools * 100\n",
    "    \n",
    "    return stats\n",
    "\n",
    "# Total schools count\n",
    "total_vienna_schools = len(df_vienna)\n",
    "total_austria_schools = len(df_austria_rest)\n",
    "\n",
    "# Calculate stats for Vienna and Austria\n",
    "vienna_stats = calculate_school_stats(df_vienna, total_vienna_schools)\n",
    "austria_stats = calculate_school_stats(df_austria_rest, total_austria_schools)\n",
    "\n",
    "# Merge the Vienna and Austria stats into a single DataFrame\n",
    "merged_stats = vienna_stats.merge(austria_stats, on='TYP', suffixes=('_Vienna', '_Austria'))\n",
    "\n",
    "# Sort the data by 'TYP' (Category) if needed\n",
    "merged_stats = merged_stats.sort_values('TYP')\n",
    "\n",
    "# Calculate total number of private schools and the percentage of private schools\n",
    "total_vienna_private = (vienna_stats['Count'] * vienna_stats['% Private'] / 100).sum()\n",
    "vienna_private_percentage = (total_vienna_private / vienna_totals['Count']) * 100\n",
    "\n",
    "total_austria_private = (austria_stats['Count'] * austria_stats['% Private'] / 100).sum()\n",
    "austria_private_percentage = (total_austria_private / austria_totals['Count']) * 100\n",
    "\n",
    "# Update the LaTeX table with the correct totals\n",
    "latex_table = r\"\"\"\n",
    "\\begin{table}[h!]\n",
    "\\centering\n",
    "\\resizebox{\\textwidth}{!}{\n",
    "\\begin{tabular}{l *{2}{rrr}}\n",
    "    \\toprule\n",
    "    \\multirow{2}{*}{\\makecell[l]{Category}} & \\multicolumn{3}{c}{Vienna} & \\multicolumn{3}{c}{Rest of Austria} \\\\\n",
    "    \\cmidrule(lr){2-4} \\cmidrule(lr){5-7}\n",
    "    & Schools & \\% Private & \\% of Schools & Schools & \\% Private & \\% of Schools \\\\\n",
    "    \\midrule\n",
    "\"\"\"\n",
    "\n",
    "# Add rows for each school type\n",
    "for _, row in merged_stats.iterrows():\n",
    "    latex_table += f\"    {row['TYP']} & {int(row['Count_Vienna'])} & {row['% Private_Vienna']:.1f} & {row['% of Total_Vienna']:.1f} & {int(row['Count_Austria'])} & {row['% Private_Austria']:.1f} & {row['% of Total_Austria']:.1f} \\\\\\\\\\n\"\n",
    "\n",
    "# Add a thin line before the total row\n",
    "latex_table += r\"    \\midrule\\n\"\n",
    "\n",
    "# Add the total row with correct private percentage\n",
    "latex_table += f\"    Total & {int(vienna_totals['Count'])} & {vienna_private_percentage:.1f} & {vienna_totals['% of Total']:.1f} & {int(austria_totals['Count'])} & {austria_private_percentage:.1f} & {austria_totals['% of Total']:.1f} \\\\\\\\\\n\"\n",
    "\n",
    "latex_table += r\"\"\"\n",
    "    \\bottomrule\n",
    "\\end{tabular}}\n",
    "\\caption{Distribution of Schools in Vienna and Austria in 2022/2023, based on \"Schools in Austria\" open dataset}\n",
    "\\label{tab:schools}\n",
    "\\end{table}\n",
    "\"\"\"\n",
    "\n",
    "# Print or save the LaTeX table code\n",
    "print(latex_table)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae6b100",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Private vs. Public \n",
    "# Add new column 'school_status' based on the ending of 'school_type_txt'\n",
    "df['ERHALTER'] = df['school_type_txt'].apply(lambda x: 'privat' if x.endswith('(privat)') else 'öffentl.')\n",
    "print(df['ERHALTER'].value_counts())\n",
    "print(df_austria['ERHALTER'].value_counts())\n",
    "\n",
    "### Comparison to rest of Austria\n",
    "vienna_counts = df['school_type_mapping'].value_counts().rename('Vienna_count')\n",
    "austria_counts = df_austria['school_type_mapping'].value_counts().rename('Austria_count')\n",
    "\n",
    "# Calculate the relative percentages\n",
    "vienna_percentages = (vienna_counts / vienna_counts.sum() * 100).rename('Vienna_percentage')\n",
    "austria_percentages = (austria_counts / austria_counts.sum() * 100).rename('Austria_percentage')\n",
    "\n",
    "# Combine the counts and percentages into a single DataFrame\n",
    "comparison_df = pd.concat([vienna_counts, vienna_percentages, austria_counts, austria_percentages], axis=1).fillna(0)\n",
    "\n",
    "# Print the schooltype comparative statistics\n",
    "print(comparison_df)\n",
    "\n",
    "# Now the occurrences of each school type by public and private categories\n",
    "vienna_counts_erhalter = df.groupby(['ERHALTER', 'school_type_mapping']).size().unstack(fill_value=0).stack().rename('Vienna_count')\n",
    "austria_counts_erhalter = df_austria.groupby(['ERHALTER', 'school_type_mapping']).size().unstack(fill_value=0).stack().rename('Austria_count')\n",
    "\n",
    "# Calculate the relative percentages\n",
    "vienna_totals_erhalter = df['ERHALTER'].value_counts()\n",
    "austria_totals_erhalter = df_austria['ERHALTER'].value_counts()\n",
    "\n",
    "vienna_percentages_erhalter = (vienna_counts_erhalter / vienna_totals_erhalter[vienna_counts_erhalter.index.get_level_values('ERHALTER')].values * 100).rename('Vienna_percentage')\n",
    "austria_percentages_erhalter = (austria_counts_erhalter / austria_totals_erhalter[austria_counts_erhalter.index.get_level_values('ERHALTER')].values * 100).rename('Austria_percentage')\n",
    "\n",
    "# Combine the counts and percentages into a single DataFrame\n",
    "comparison_df_erhalter = pd.concat([vienna_counts_erhalter, vienna_percentages_erhalter, austria_counts_erhalter, austria_percentages_erhalter], axis=1).fillna(0)\n",
    "\n",
    "# Reset index to have a proper DataFrame structure\n",
    "comparison_df_erhalter.reset_index(inplace=True)\n",
    "\n",
    "# Print the comparative statistics\n",
    "print(comparison_df_erhalter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec66706a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## A number of schools in Vienna are extensions of other schools. These have the same name, a different address, and the same school ID (SKZ) as the original school. \n",
    "df_extensions = df[df['SKZ'].str.endswith('e')]\n",
    "\n",
    "# Calculate the number of extensions for each base school\n",
    "extensions_count = df_extensions.groupby(df['SKZ'].str.rstrip('e')).size()\n",
    "for skz in extensions_count.index:\n",
    "    if skz in df['SKZ'].values:\n",
    "        extensions_count[skz] += 1\n",
    "\n",
    "# Sort the extensions_count by SKZ\n",
    "extensions_count = extensions_count.sort_values(ascending=False)\n",
    "\n",
    "total_sum_extensions = extensions_count.sum()\n",
    "\n",
    "# Calculate descriptive statistics\n",
    "average_extensions = extensions_count.mean()\n",
    "std_dev_extensions = extensions_count.std()\n",
    "\n",
    "# Print results\n",
    "print(\"\\nTotal Sum of School Numbers in Extensions Count:\")\n",
    "print(total_sum_extensions)\n",
    "print(\"Extensions Count:\")\n",
    "print(extensions_count)\n",
    "print(\"\\nDescriptive Statistics:\")\n",
    "print(f\"Average number of extensions: {average_extensions}\")\n",
    "print(f\"Standard deviation of extensions: {std_dev_extensions}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f572b6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"\\nSchool Type Code Frequency:\")\n",
    "print(df['school_type_code'].value_counts())\n",
    "print(df['school_type_txt'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bfabc8a",
   "metadata": {},
   "source": [
    "### Part 2.4: Website URL updating and analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa59216d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import aiohttp\n",
    "import asyncio\n",
    "from datetime import datetime\n",
    "from aiohttp import ClientSession\n",
    "from urllib.parse import urlparse, urlunparse\n",
    "import nest_asyncio\n",
    "\n",
    "# Apply nest_asyncio to allow nested event loops in Jupyter notebooks\n",
    "nest_asyncio.apply()\n",
    "\n",
    "def normalize_url(url):\n",
    "    if url == \"No URL\":\n",
    "        return url\n",
    "    parsed_url = urlparse(url)\n",
    "    if not parsed_url.scheme:\n",
    "        parsed_url = parsed_url._replace(scheme=\"http\")\n",
    "    if parsed_url.scheme not in [\"http\", \"https\"]:\n",
    "        return \"Invalid URL\"\n",
    "    return urlunparse(parsed_url)\n",
    "\n",
    "async def fetch_url(session, url):\n",
    "    try:\n",
    "        async with session.get(url, allow_redirects=True) as response:\n",
    "            if response.status == 200:\n",
    "                return str(response.url)\n",
    "            else:\n",
    "                return \"Invalid URL\"\n",
    "    except Exception as e:\n",
    "        return \"Invalid URL\"\n",
    "\n",
    "async def update_urls(df):\n",
    "    async with ClientSession() as session:\n",
    "        tasks = []\n",
    "        for index, row in df.iterrows():\n",
    "            url = normalize_url(row['website_url'])\n",
    "            if url not in [\"No URL\", \"Invalid URL\"]:\n",
    "                tasks.append(fetch_url(session, url))\n",
    "            else:\n",
    "                tasks.append(asyncio.sleep(0))  # Dummy coroutine for 'No URL' and 'Invalid URL'\n",
    "        \n",
    "        results = await asyncio.gather(*tasks)\n",
    "        # Ensure \"No URL\" is handled correctly in the updated DataFrame\n",
    "        for i, result in enumerate(results):\n",
    "            if df.at[i, 'website_url'] == \"No URL\":\n",
    "                results[i] = \"No URL\"\n",
    "        df['updated_URL'] = results\n",
    "    return df\n",
    "\n",
    "# Function to get netloc from URL and remove www prefix\n",
    "def get_netloc(url):\n",
    "    if url in [\"No URL\", \"Invalid URL\"]:\n",
    "        return url\n",
    "    parsed_url = urlparse(url)\n",
    "    netloc = parsed_url.netloc\n",
    "    if netloc and netloc.startswith(\"www.\"):\n",
    "        netloc = netloc[4:]\n",
    "    return netloc\n",
    "\n",
    "# Calculate statistics\n",
    "def get_statistics(df, updated_df):\n",
    "    total_urls = len(df)\n",
    "    no_url_count = (df['website_url'] == 'No URL').sum()\n",
    "    invalid_url_count = (updated_df['updated_URL'] == 'Invalid URL').sum()\n",
    "    valid_url_count = total_urls - no_url_count - invalid_url_count\n",
    "    \n",
    "    redirected_count = (df['website_url'].apply(get_netloc) != updated_df['updated_URL'].apply(get_netloc)).sum()\n",
    "    \n",
    "    stats = {\n",
    "        'total_urls': total_urls,\n",
    "        'no_url_count': no_url_count,\n",
    "        'invalid_url_count': invalid_url_count,\n",
    "        'valid_url_count': valid_url_count,\n",
    "        'redirected_count': redirected_count\n",
    "    }\n",
    "    \n",
    "    return stats\n",
    "\n",
    "# Load your actual DataFrame 'df' here\n",
    "# df = pd.read_csv('your_dataframe.csv')  # Example of loading your actual DataFrame\n",
    "\n",
    "# Run the async update function\n",
    "loop = asyncio.get_event_loop()\n",
    "updated_df = loop.run_until_complete(update_urls(df))\n",
    "\n",
    "# Display the updated DataFrame\n",
    "print(updated_df)\n",
    "\n",
    "# Get statistics\n",
    "stats = get_statistics(df, updated_df)\n",
    "\n",
    "# Display the statistics\n",
    "print(\"Statistics:\")\n",
    "for key, value in stats.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "\n",
    "# Compare original and updated DataFrames\n",
    "comparison_df = df.copy()\n",
    "comparison_df['updated_URL'] = updated_df['updated_URL']\n",
    "comparison_df['netloc_changed'] = comparison_df.apply(\n",
    "    lambda row: get_netloc(row['website_url']) != get_netloc(row['updated_URL']), axis=1\n",
    ")\n",
    "\n",
    "print(\"\\nComparison DataFrame:\")\n",
    "print(comparison_df)\n",
    "\n",
    "# Save the comparison DataFrame to CSV to check out all rows\n",
    "comparison_df.to_csv(f\"2_comparison_df_{datetime.now().strftime('%Y-%m-%d')}.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db62402",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading old data\n",
    "\n",
    "# Specify the path to the CSV file\n",
    "csv_path = './2_comparison_df_2024-08-12.csv'\n",
    "\n",
    "# Read the CSV file into a dataframe\n",
    "comparison_df = pd.read_csv(csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7e69be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descriptive statistics of link availability\n",
    "netloc_changed_counts = comparison_df[\"netloc_changed\"].value_counts()\n",
    "updated_URL_invalid_count = comparison_df[\"updated_URL\"].apply(lambda x: x == \"Invalid URL\").sum()\n",
    "total_urls = comparison_df.shape[0]\n",
    "\n",
    "# Percentage of netloc_changed by school type\n",
    "school_type_group = comparison_df.groupby(\"school_type_txt\")[\"netloc_changed\"].mean() * 100\n",
    "\n",
    "# Output descriptive statistics\n",
    "print(\"Netloc Changed Counts:\\n\", netloc_changed_counts)\n",
    "print(\"Number of Invalid URLs in updated_URL:\", updated_URL_invalid_count)\n",
    "print(\"Total Number of URLs:\", total_urls)\n",
    "print(\"Percentage of netloc_changed by school type:\\n\", school_type_group)\n",
    "\n",
    "# Grouping data by school type\n",
    "grouped = comparison_df.groupby(\"school_type_txt\").agg(\n",
    "    netloc_changed_percentage=(\"netloc_changed\", \"mean\"),\n",
    "    observations=(\"netloc_changed\", \"count\")\n",
    ")\n",
    "\n",
    "grouped[\"netloc_changed_percentage\"] = (grouped[\"netloc_changed_percentage\"] * 100).round(2)\n",
    "\n",
    "# Calculate private and non-private school statistics\n",
    "private_schools = comparison_df[comparison_df[\"ERHALTER\"] == \"privat\"]\n",
    "non_private_schools = comparison_df[comparison_df[\"ERHALTER\"] != \"privat\"]\n",
    "\n",
    "private_netloc_changed_count = private_schools[\"netloc_changed\"].sum()\n",
    "private_total_count = private_schools.shape[0]\n",
    "private_netloc_changed_percentage = (private_netloc_changed_count / private_total_count * 100).round(2)\n",
    "\n",
    "non_private_netloc_changed_count = non_private_schools[\"netloc_changed\"].sum()\n",
    "non_private_total_count = non_private_schools.shape[0]\n",
    "non_private_netloc_changed_percentage = (non_private_netloc_changed_count / non_private_total_count * 100).round(2)\n",
    "\n",
    "# Prepare the LaTeX table\n",
    "latex_table = grouped.to_latex(index=True, \n",
    "                               header=[\"% Netloc Changed\", \"Observations\"], \n",
    "                               caption=\"Netloc Change Statistics by School Type (2021-2024)\", \n",
    "                               label=\"tab:netloc_change_stats\",\n",
    "                               column_format=\"lcc\", # Column alignment: left, center, center\n",
    "                               formatters={\"netloc_changed_percentage\": \"{:.2f}\\%\".format},\n",
    "                               escape=False)\n",
    "\n",
    "# Append private and non-private school statistics\n",
    "latex_table += \"\\\\hline\\n\"\n",
    "latex_table += f\"Private Schools & {private_netloc_changed_percentage:.2f}\\\\% & {private_total_count} \\\\\\\\\\n\"\n",
    "latex_table += f\"Non-Private Schools & {non_private_netloc_changed_percentage:.2f}\\\\% & {non_private_total_count} \\\\\\\\\\n\"\n",
    "\n",
    "# Output the LaTeX table\n",
    "print(latex_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3ad43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "# Assuming `comparison_df` is already loaded and processed as before\n",
    "\n",
    "# Calculate the total number of schools\n",
    "total_schools = len(comparison_df)\n",
    "\n",
    "# Calculate the number of schools with no URL\n",
    "no_url = comparison_df[\"website_url\"].apply(lambda x: x == \"No URL\").sum()\n",
    "\n",
    "# Calculate the percentage of schools with no URL\n",
    "percentage_no_url = (no_url / total_schools) * 100\n",
    "\n",
    "# Clean up the domains\n",
    "def clean_domain(url):\n",
    "    if url == \"No URL\":\n",
    "        return None\n",
    "    parsed_url = urlparse(url)\n",
    "    domain = parsed_url.netloc\n",
    "    if domain.startswith(\"www.\"):\n",
    "        domain = domain[4:]\n",
    "    return domain.rstrip('/')\n",
    "\n",
    "# Apply the domain cleaning function\n",
    "comparison_df['clean_domain'] = comparison_df['website_url'].apply(clean_domain)\n",
    "comparison_df['cleaned_updated_URL'] = comparison_df['updated_URL'].apply(clean_domain)\n",
    "\n",
    "# Drop rows where the clean_domain is None (for No URL)\n",
    "valid_domains = comparison_df.dropna(subset=['clean_domain'])\n",
    "\n",
    "# Count the number of unique domains\n",
    "unique_domains_count = valid_domains['clean_domain'].nunique()\n",
    "\n",
    "# Create a dataframe for unique websites\n",
    "unique_domains_df = valid_domains.drop_duplicates(subset=['clean_domain'])\n",
    "\n",
    "# Calculate the number of domain changes for all websites\n",
    "domain_changes_all = valid_domains['netloc_changed'].sum()\n",
    "\n",
    "# Calculate the number of domain changes for unique websites\n",
    "domain_changes_unique = unique_domains_df['netloc_changed'].sum()\n",
    "\n",
    "# Calculate the number of unique domains that became unavailable\n",
    "unavailable_domains_count = unique_domains_df[unique_domains_df[\"updated_URL\"] == \"Invalid URL\"].shape[0]\n",
    "\n",
    "# Calculate the number of schule.wien.at domains that became unavailable\n",
    "schule_unavailable_count = unique_domains_df[(unique_domains_df[\"updated_URL\"] == \"Invalid URL\") & \n",
    "                                             (unique_domains_df[\"clean_domain\"].str.endswith(\"schule.wien.at\"))].shape[0]\n",
    "\n",
    "# Calculate the number of unique domains migrating\n",
    "# Note: This row only has the total column filled with the unique domains number\n",
    "unique_domains_migrated = domain_changes_unique - unavailable_domains_count\n",
    "\n",
    "# Calculate the number of domains migrated to schule.wien.at\n",
    "migrated_to_schule_count = unique_domains_df[(unique_domains_df[\"netloc_changed\"]) & unique_domains_df[\"cleaned_updated_URL\"].str.endswith(\"schule.wien.at\")].shape[0]\n",
    "\n",
    "# Calculate the number of domains migrated to another unique domain (not schule.wien.at)\n",
    "migrated_to_other_domain_count = unique_domains_migrated - migrated_to_schule_count\n",
    "\n",
    "# Calculate the number of domains that migrated from and to schule.wien.at with redirection configured\n",
    "migrated_within_schule_count = unique_domains_df[(unique_domains_df[\"clean_domain\"].str.endswith(\"schule.wien.at\")) &\n",
    "                                                 (unique_domains_df[\"netloc_changed\"]) & (unique_domains_df[\"cleaned_updated_URL\"].str.endswith(\"schule.wien.at\"))].shape[0]\n",
    "\n",
    "# Prepare the LaTeX snippet\n",
    "latex_snippet = f\"\"\"\n",
    "\\\\begin{{table}}[ht]\n",
    "\\\\centering\n",
    "\\\\caption{{Netloc Changes from 15.4.2021 to 13.8.2024}}\n",
    "\\\\label{{tab:netloc_changes_domains}}\n",
    "\\\\begin{{tabular}}{{lrrr}}\n",
    "\\\\toprule\n",
    "URL Availability Measure & Observations & Total & Percentage \\\\\\\\\n",
    "\\\\midrule\n",
    "Total number of schools in dataset & {total_schools} & {total_schools} & 100\\\\% \\\\\\\\\n",
    "Schools with a website & {total_schools - no_url} & {total_schools} & {100 - percentage_no_url:.2f}\\\\% \\\\\\\\\n",
    "\\\\quad Domain changes of all websites & {domain_changes_all} & {total_schools - no_url} & {domain_changes_all / (total_schools - no_url) * 100:.2f}\\\\% \\\\\\\\\n",
    "Unique domains & {unique_domains_count} & {total_schools} & {unique_domains_count / total_schools * 100:.2f}\\\\% \\\\\\\\\n",
    "\\\\quad Domain changes of unique websites & {domain_changes_unique} & {unique_domains_count} & {domain_changes_unique / unique_domains_count * 100:.2f}\\\\% \\\\\\\\\n",
    "\\\\quad Unique domains becoming unavailable & {unavailable_domains_count} & {domain_changes_unique} & {unavailable_domains_count / domain_changes_unique * 100:.2f}\\\\% \\\\\\\\\n",
    "\\\\quad schule.wien.at domains becoming unavailable & {schule_unavailable_count} & {unavailable_domains_count} & {schule_unavailable_count / unavailable_domains_count * 100:.2f}\\\\% \\\\\\\\\n",
    "\\\\midrule\n",
    "Unique domains migrating &  & {unique_domains_migrated} & \\\\\\\\\n",
    "\\\\quad Migrated to schule.wien.at domain & {migrated_to_schule_count} & {unique_domains_migrated} & {migrated_to_schule_count / unique_domains_migrated * 100:.2f}\\\\% \\\\\\\\\n",
    "\\\\quad Migrated to another unique domain & {migrated_to_other_domain_count} & {unique_domains_migrated} & {migrated_to_other_domain_count / unique_domains_migrated * 100:.2f}\\\\% \\\\\\\\\n",
    "\\\\quad Migrated from and to a schule.wien.at domain & {migrated_within_schule_count} & {unique_domains_migrated} & {migrated_within_schule_count / unique_domains_migrated * 100:.2f}\\\\% \\\\\\\\\n",
    "\\\\bottomrule\n",
    "\\\\end{{tabular}}\n",
    "\\\\end{{table}}\n",
    "\"\"\"\n",
    "\n",
    "print(latex_snippet)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fef6e40",
   "metadata": {},
   "source": [
    "## Part 3: Extracting data from websites\n",
    "There are four possible approaches:\n",
    "- Get data from one website homepage\n",
    "- Get data from whole website domain\n",
    "- Get data from a series of websites from a .csv file. \n",
    "- Get historic data from a series of websites, based on a certain cut-off date. \n",
    "\n",
    "The output is stored in a .csv format so it can be analysed in the next stage, where the variables can be picked up again and transformed. \n",
    "\n",
    "Known issues with sites:\n",
    "- some domains may be hijacked (e.g. Poker domains show up in list)\n",
    "- count every external link per domain only once\n",
    "- some schools have several domains\n",
    "- some schools have an updated website\n",
    "\n",
    "Here are some examples of interesting sites:\n",
    "- https://www.wien.gv.at/bildung/schulen/modeschule/kooperationen/forschungsprojekt-oeaw.html\n",
    "- https://portal.billroth73.at\n",
    "- https://eduthek.at\n",
    "- https://www.fms15.at/impressum/\n",
    "\n",
    "\n",
    "Some possible improvements: \n",
    "- Create iterative approach where those websites from comparison_df that have no data yet are scraped in a second step and the resulting data df has to be updated with new pages found. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08700072",
   "metadata": {},
   "outputs": [],
   "source": [
    "## MASTER A website scraper that takes links as an input, visits sites, and \n",
    "\n",
    "input_df = comparison_df\n",
    "\n",
    "import aiohttp\n",
    "import asyncio\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import json\n",
    "from urllib.parse import urljoin, urlparse\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "# Main async function to SCRAPE ALL URLs from a DataFrame\n",
    "async def scrape_all(df):\n",
    "    results = []\n",
    "    domain_last_visited = {}\n",
    "    \n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        print(\"session: \")\n",
    "        print(session)\n",
    "        tasks = []\n",
    "        for index, row in df.iterrows():\n",
    "            base_url = row[\"updated_URL\"]\n",
    "            skz = row[\"SKZ\"]\n",
    "            \n",
    "            if base_url in [\"No URL\", \"Invalid URL\"]:\n",
    "                print(f\"Skipping {skz}: {base_url}\")\n",
    "                continue\n",
    "\n",
    "            print(f\"Scheduling scrape for {base_url}...\")\n",
    "\n",
    "            task = asyncio.create_task(scrape_website(session, base_url, skz, domain_last_visited))\n",
    "            tasks.append(task)\n",
    "        \n",
    "        results = await asyncio.gather(*tasks)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Function to scrape a single website\n",
    "async def scrape_website(session, base_url, skz, domain_last_visited):\n",
    "    visited_internal_links = set()\n",
    "    website_internal_links = set()\n",
    "    website_all_links = set()\n",
    "    attempted_visits_links = set()\n",
    "\n",
    "    # Scrape the homepage\n",
    "    base_url = base_url[:-1] if base_url.endswith(\"/\") else base_url\n",
    "    homepage_internal_links, homepage_all_links, visited_link = await scrape_links(session, base_url, domain_last_visited)\n",
    "    attempted_visits_links.add(base_url)\n",
    "    visited_internal_links.add(visited_link)\n",
    "    # print(f\"added {base_url} to visited_internal_links\")\n",
    "    website_internal_links.update(homepage_internal_links)\n",
    "    website_all_links.update(homepage_all_links)\n",
    "\n",
    "    # Scrape the internal links found on the homepage\n",
    "    tasks = []\n",
    "    for internal_link in homepage_internal_links:\n",
    "        task = asyncio.create_task(scrape_links(session, internal_link, domain_last_visited))\n",
    "        tasks.append(task)\n",
    "        attempted_visits_links.add(internal_link)\n",
    "        # print(f\"added {internal_link} to attempted_visits_links\")\n",
    "        await asyncio.sleep(7)  # Ensure delay between internal page visits\n",
    "    \n",
    "    results = await asyncio.gather(*tasks)\n",
    "    \n",
    "    for internal_links, external_links, visited_internal_link in results:\n",
    "        visited_internal_links.add(visited_internal_link)\n",
    "        website_internal_links.update(internal_links)\n",
    "        website_all_links.update(external_links)\n",
    "\n",
    "    # # Here I would add the code for visiting the internal links found on the internal links\n",
    "    # # Don't forget to change the filename of the output file\n",
    "    # links_to_visit = website_internal_links - visited_internal_links\n",
    "    # tasks = []\n",
    "    # for internal_link in links_to_visit:\n",
    "    #     task = asyncio.create_task(scrape_links(session, internal_link, domain_last_visited))\n",
    "    #     tasks.append(task)\n",
    "    #     attempted_visits_links.add(internal_link)\n",
    "    #     print(f\"added {internal_link} to attempted_visits_links\")\n",
    "    #     await asyncio.sleep(7)  # Ensure delay between internal page visits\n",
    "    # results = await asyncio.gather(*tasks)\n",
    "    # for internal_links, external_links, visited_internal_link in results:\n",
    "    #     visited_internal_links.add(visited_internal_link)\n",
    "    #     website_internal_links.update(internal_links)\n",
    "    #     website_all_links.update(external_links)\n",
    "    \n",
    "    print(list(visited_internal_links))\n",
    "\n",
    "    return {\n",
    "        \"SKZ\": skz,\n",
    "        \"base_url\": base_url,\n",
    "        \"internal_links\": list(website_internal_links),\n",
    "        \"visited_internal_links\": list(visited_internal_links),\n",
    "        \"attempted_visits_links\": list(attempted_visits_links),\n",
    "        \"all_links\": list(website_all_links),\n",
    "    }\n",
    "\n",
    "async def scrape_links(session, url, domain_last_visited):\n",
    "    print(f\"Visiting {url}\")\n",
    "\n",
    "    parsed_url = urlparse(url)\n",
    "    base_url = parsed_url.scheme + \"://\" + parsed_url.netloc\n",
    "    # from https to http, or vice versa\n",
    "    base_url_alternative_scheme = base_url.replace(\"http://\", \"https://\") if base_url.startswith(\"http://\") else base_url.replace(\"https://\", \"http://\")\n",
    "    # from www to non-www, or vice versa\n",
    "    base_url_www_switch = base_url\n",
    "    base_url_alternative_scheme_www_switch = base_url_alternative_scheme\n",
    "\n",
    "    if base_url.startswith(\"http://www.\") or base_url.startswith(\"https://www.\"):\n",
    "        base_url_www_switch = base_url.replace(\"www.\", \"\")\n",
    "        base_url_alternative_scheme_www_switch = base_url_alternative_scheme.replace(\"www.\", \"\")\n",
    "    elif base_url.startswith(\"http\"):\n",
    "        base_url_www_switch = base_url.replace(\"http://\", \"http://www.\")\n",
    "        base_url_alternative_scheme_www_switch = base_url.replace(\"https://\", \"https://www.\")\n",
    "    else:\n",
    "        print(f\"Invalid base_url: {base_url}\")\n",
    "\n",
    "    html = await fetch(session, url, domain_last_visited)\n",
    "\n",
    "    if html:\n",
    "            page_internal_links = set()\n",
    "            page_all_links = set()\n",
    "            visited_link = url\n",
    "            print(f\"added {url} to visited_internal_links\")\n",
    "            soup = BeautifulSoup(html, 'html.parser')\n",
    "            for link in soup.find_all('a', href=True):\n",
    "                link_href = link['href']\n",
    "                full_url = urljoin(base_url, link_href)\n",
    "                if not is_valid_url(full_url):\n",
    "                    print(f\"Skipping invalid URL: {full_url}\")\n",
    "                    continue\n",
    "                page_all_links.add(full_url)\n",
    "                if not (full_url.startswith(base_url) or full_url.startswith(base_url_alternative_scheme) or full_url.startswith(base_url_www_switch) or full_url.startswith(base_url_alternative_scheme_www_switch)):\n",
    "                    continue\n",
    "                if link_href.startswith(('javascript:', 'tel:', 'fax:', 'mailto:')) or any(link_href.endswith(ext) for ext in non_html_extensions):\n",
    "                    continue\n",
    "                clean = clean_url(full_url, base_url)\n",
    "                page_internal_links.add(clean)\n",
    "            return list(page_internal_links), list(page_all_links), visited_link\n",
    "    return [], [], \"\"\n",
    "\n",
    "\n",
    "# Function to remove URL fragments and queries\n",
    "def clean_url(url, base_url):\n",
    "    parsed = urlparse(url)\n",
    "    netloc = parsed.netloc.split(':')[0]\n",
    "    parsed = parsed._replace(netloc=netloc)\n",
    "    #parsed = parsed._replace(query='')\n",
    "    parsed = parsed._replace(fragment='')\n",
    "    parsed = parsed._replace(path=parsed.path.rstrip('/'))\n",
    "    parsed_base_url = urlparse(base_url)\n",
    "    parsed = parsed._replace(scheme=parsed_base_url.scheme)\n",
    "    parsed = parsed._replace(netloc=parsed_base_url.netloc)\n",
    "    return parsed.geturl()\n",
    "\n",
    "def is_valid_url(url):\n",
    "    try:\n",
    "        result = urlparse(url)\n",
    "        return all([result.scheme, result.netloc])\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "non_html_extensions = ['.jpg', '.jpeg', '.png', '.gif', '.bmp', '.svg', '.webp', '.ico', \n",
    "                       '.pdf', '.doc', '.docx', '.xls', '.xlsx', '.ppt', '.pptx', \n",
    "                       '.zip', '.rar', '.tar', '.gz', '.mp3', '.wav', '.mp4', '.avi']\n",
    "\n",
    "# Function to FETCH and parse HTML content\n",
    "async def fetch(session, url, domain_last_visited):\n",
    "    domain = urlparse(url).netloc\n",
    "    # Ensure at least 7 seconds between visits to the same domain\n",
    "    if domain in domain_last_visited:\n",
    "        elapsed_time = time.time() - domain_last_visited[domain]\n",
    "        if elapsed_time < 7:\n",
    "            await asyncio.sleep(7 - elapsed_time)\n",
    "    try:\n",
    "        async with session.get(url) as response:\n",
    "            domain_last_visited[domain] = time.time()\n",
    "            if response.status == 200:\n",
    "                return await response.text()\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Function to save results to JSON\n",
    "def save_results(results):\n",
    "    date_str = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "    filename = f\"3_scraped_links_{date_str}.json\"\n",
    "    with open(filename, 'w') as f:\n",
    "        json.dump(results, f, indent=4)\n",
    "    print(f\"Results saved to {filename}\")\n",
    "\n",
    "# Run the scraper\n",
    "results = asyncio.run(scrape_all(input_df))\n",
    "\n",
    "print(results)\n",
    "\n",
    "# Save the results\n",
    "save_results(results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49579898",
   "metadata": {},
   "source": [
    "## Part 4: Cleaning and Transforming the data\n",
    "We need to go through several steps: \n",
    "- Remove schools without a website\n",
    "- remove duplicate websites\n",
    "- Report on difference\n",
    "- \n",
    "\n",
    "Known issues with sites:\n",
    "- some domains may be hijacked (e.g. Poker domains show up in list)\n",
    "- count every external link per domain only once\n",
    "- some schools have several domains\n",
    "- some schools have an updated website\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64013229",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.1 Comparative statistics about analysed data. \n",
    "# Loading of json file with links into a DataFrame\n",
    "path = \"./3_scraped_links_2024-08-12.json\"\n",
    "with open(path, 'r') as f:\n",
    "    results = json.load(f)\n",
    "\n",
    "# Create a DataFrame from the results\n",
    "df_results = pd.DataFrame(results)\n",
    "\n",
    "df_results.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bcf877f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Example data\n",
    "{\n",
    "    \"SKZ\": \"922012\",\n",
    "    \"base_url\": \"https://gts-anton-sattler-gasse.schule.wien.at\",\n",
    "    \"internal_links\": [\n",
    "        \"https://gts-anton-sattler-gasse.schule.wien.at/datenschutz\",\n",
    "        \"https://gts-anton-sattler-gasse.schule.wien.at/administration\",\n",
    "        \"https://gts-anton-sattler-gasse.schule.wien.at/schwerpunkte\",\n",
    "        \"https://gts-anton-sattler-gasse.schule.wien.at/elternverein\",\n",
    "    ],\n",
    "    \"visited_internal_links\": [\n",
    "        \"https://gts-anton-sattler-gasse.schule.wien.at/datenschutz\",\n",
    "        \"\",\n",
    "        \"https://gts-anton-sattler-gasse.schule.wien.at/schwerpunkte\",\n",
    "    ],\n",
    "    \"attempted_visits_links\": [\n",
    "        \"https://gts-anton-sattler-gasse.schule.wien.at/datenschutz\",\n",
    "        \"https://gts-anton-sattler-gasse.schule.wien.at/administration\",\n",
    "        \"https://gts-anton-sattler-gasse.schule.wien.at/schwerpunkte\",\n",
    "    ],\n",
    "    \"all_links\": [\n",
    "        \"http://www.lehrerweb.at\",\n",
    "        \"https://youtu.be/tuvt4R3ozg8\",\n",
    "        \"https://gts-anton-sattler-gasse.schule.wien.at/administration\",\n",
    "        \"https://gts-anton-sattler-gasse.schule.wien.at/schwerpunkte\",\n",
    "        \"https://gts-anton-sattler-gasse.schule.wien.at/elternverein\",\n",
    "        \"https://gts-anton-sattler-gasse.schule.wien.at/puma\",\n",
    "        \"https://gts-anton-sattler-gasse.schule.wien.at/about\",\n",
    "        \"https://www.schule.wien.at/kontaktformular-fuer-e-mails/?no_cache=1&tx_email2powermail%5Bid%5D=327\",\n",
    "        \"http://puma.lehrerweb.at\",\n",
    "        \"https://gts-anton-sattler-gasse.schule.wien.at/\",\n",
    "        \"https://gts-anton-sattler-gasse.schule.wien.at\",\n",
    "        \"https://www.bmbwf.gv.at/Themen/schule/schulrecht/ds.html\",\n",
    "        \"https://privacy.microsoft.com/de-de/privacystatement\",\n",
    "    ]\n",
    "},"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc459f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Creating DataFrames\n",
    "links_df = df_results\n",
    "schools_df = comparison_df\n",
    "\n",
    "# Convert 'SKZ' to string for proper merging\n",
    "links_df['SKZ'] = links_df['SKZ'].astype(str)\n",
    "schools_df['SKZ'] = schools_df['SKZ'].astype(str)\n",
    "\n",
    "# Total Number of Websites\n",
    "total_websites = links_df['SKZ'].nunique()\n",
    "print(f\"Total Number of Websites: {total_websites}\")\n",
    "\n",
    "# Websites with Data vs. No Data\n",
    "links_df['has_data'] = links_df['all_links'].apply(lambda x: len(x) > 0)\n",
    "websites_with_data = links_df['has_data'].sum()\n",
    "websites_without_data = len(links_df) - websites_with_data\n",
    "print(f\"Websites with Data: {websites_with_data}\")\n",
    "print(f\"Websites without Data: {websites_without_data}\")\n",
    "\n",
    "# Total Number of Links\n",
    "total_links = links_df['all_links'].apply(len).sum()\n",
    "print(f\"Total Number of Links: {total_links}\")\n",
    "\n",
    "# Internal vs. External Links\n",
    "total_internal_links = links_df['internal_links'].apply(len).sum()\n",
    "total_external_links = total_links - total_internal_links\n",
    "print(f\"Total Internal Links: {total_internal_links}\")\n",
    "print(f\"Total External Links: {total_external_links}\")\n",
    "\n",
    "# Average Number of Links per Website\n",
    "average_total_links_per_website = links_df['all_links'].apply(len).mean()\n",
    "average_internal_links_per_website = links_df['internal_links'].apply(len).mean()\n",
    "average_external_links_per_website = average_total_links_per_website - average_internal_links_per_website\n",
    "print(f\"Average Number of Total Links per Website: {average_total_links_per_website:.2f}\")\n",
    "print(f\"Average Number of Internal Links per Website: {average_internal_links_per_website:.2f}\")\n",
    "print(f\"Average Number of External Links per Website: {average_external_links_per_website:.2f}\")\n",
    "\n",
    "# Merge DataFrames to analyze by school type\n",
    "merged_df = pd.merge(links_df, schools_df, on='SKZ', how='left')\n",
    "\n",
    "# Analysis by School Type\n",
    "school_type_group = merged_df.groupby('school_type_txt')\n",
    "\n",
    "# Aggregate functions for groupby\n",
    "def total_links_func(x):\n",
    "    return sum(len(links) for links in x)\n",
    "\n",
    "def average_links_func(x):\n",
    "    return sum(len(links) for links in x) / len(x)\n",
    "\n",
    "# Compute aggregated statistics\n",
    "school_type_analysis = school_type_group.agg(\n",
    "    total_websites=('SKZ', 'nunique'),\n",
    "    websites_with_data=('has_data', 'sum'),\n",
    "    websites_without_data=('has_data', lambda x: len(x) - x.sum()),\n",
    "    total_links=('all_links', total_links_func),\n",
    "    total_internal_links=('internal_links', total_links_func),\n",
    "    average_total_links_per_website=('all_links', average_links_func),\n",
    "    average_internal_links_per_website=('internal_links', average_links_func)\n",
    ")\n",
    "\n",
    "# Calculate total external links and average external links per website\n",
    "school_type_analysis['total_external_links'] = school_type_analysis['total_links'] - school_type_analysis['total_internal_links']\n",
    "school_type_analysis['average_external_links_per_website'] = school_type_analysis['average_total_links_per_website'] - school_type_analysis['average_internal_links_per_website']\n",
    "\n",
    "print(school_type_analysis)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5160ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning the links\n",
    "\n",
    "# open the json file with the links as a dataframe\n",
    "import pandas as pd\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Load the JSON file into a DataFrame\n",
    "path = \"./scraped_links_2024-08-12.json\"\n",
    "with open(path, 'r') as f:\n",
    "    results = json.load(f)\n",
    "\n",
    "# Create a DataFrame from the results\n",
    "links_df = pd.DataFrame(results)\n",
    "\n",
    "# Remove unwanted links and clean the remaining links\n",
    "\n",
    "{\n",
    "        \"SKZ\": \"922012\",\n",
    "        \"base_url\": \"https://gts-anton-sattler-gasse.schule.wien.at\",\n",
    "        \"internal_links\": [\n",
    "            \"https://gts-anton-sattler-gasse.schule.wien.at/datenschutz\",\n",
    "            \"https://gts-anton-sattler-gasse.schule.wien.at/administration\",\n",
    "            \"https://gts-anton-sattler-gasse.schule.wien.at/schwerpunkte\",\n",
    "            \"https://gts-anton-sattler-gasse.schule.wien.at/elternverein\",\n",
    "            \"https://gts-anton-sattler-gasse.schule.wien.at/puma\",\n",
    "        ],\n",
    "        \"visited_internal_links\": [\n",
    "            \"https://gts-anton-sattler-gasse.schule.wien.at/datenschutz\",\n",
    "            \"\",\n",
    "            \"https://gts-anton-sattler-gasse.schule.wien.at/schwerpunkte\",\n",
    "\n",
    "        ],\n",
    "        \"attempted_visits_links\": [\n",
    "            \"https://gts-anton-sattler-gasse.schule.wien.at/datenschutz\",\n",
    "            \"https://gts-anton-sattler-gasse.schule.wien.at/administration\",\n",
    "            \"https://gts-anton-sattler-gasse.schule.wien.at/schwerpunkte\",\n",
    "            \"https://gts-anton-sattler-gasse.schule.wien.at/elternverein\",\n",
    "\n",
    "        ],\n",
    "        \"all_links\": [\n",
    "            \"http://www.lehrerweb.at\",\n",
    "            \"https://youtu.be/tuvt4R3ozg8\",\n",
    "            \"https://gts-anton-sattler-gasse.schule.wien.at/administration\",\n",
    "            \"https://gts-anton-sattler-gasse.schule.wien.at/schwerpunkte\",\n",
    "            \"https://gts-anton-sattler-gasse.schule.wien.at/elternverein\",\n",
    "            \"https://gts-anton-sattler-gasse.schule.wien.at/puma\",\n",
    "\n",
    "        ]\n",
    "    },\n",
    "\n",
    "# Remove internal links based on stripped 'base_url'\n",
    "\n",
    "\n",
    "output_data = []\n",
    "\n",
    "for i, row in links_df.iterrows():\n",
    "    parsed_base_url = urlparse(row['base_url'])\n",
    "    stripped_base_url = parsed_base_url.netloc.replace(\"www.\", \"\")\n",
    "    # stripped_base_url = row['base_url'].replace(\"http://\", \"\").replace(\"https://\", \"\").rstrip('/').replace(\"www.\", \"\")\n",
    "    # Filter out faulty links and internal links\n",
    "    cleaned_links = [link for link in row['all_links'] \n",
    "                     if not link.strip().lower().startswith((f\"http://{stripped_base_url.lower()}\", \n",
    "                                                             f\"https://{stripped_base_url.lower()}\", \n",
    "                                                             f\"http://www.{stripped_base_url.lower()}\", \n",
    "                                                             f\"https://www.{stripped_base_url.lower()}\", \n",
    "                                                             \"javascript:\", \"tel:\", \"fax:\", \"mailto:\")) \n",
    "                     and \" \" not in link]    # Remove links with spaces\n",
    "    cleaned_links = [link.replace('http://', '').replace('https://', '').replace('www.', '').split('#')[0].split('?')[0].rstrip('/') for link in cleaned_links]\n",
    "    cleaned_domains = [link.split(\"/\", 1)[0].lower() for link in cleaned_links]\n",
    "    for i, domain in enumerate(cleaned_domains):\n",
    "        # merge social media domains\n",
    "        # youtu.be -> youtube.com\n",
    "        if domain == \"youtu.be\":\n",
    "            cleaned_domains[i] = \"youtube.com\"\n",
    "        # de-de.facebook.com -> facebook.com\n",
    "        elif domain.endswith(\".facebook.com\"):\n",
    "            cleaned_domains[i] = \"facebook.com\"\n",
    "        elif domain.endswith(\"fb.com\"):\n",
    "            cleaned_domains[i] = \"facebook.com\"\n",
    "        # vm.tiktok.com -> tiktok.com\n",
    "        elif domain.endswith(\".tiktok.com\"):\n",
    "            cleaned_domains[i] = \"tiktok.com\"\n",
    "        # at.linkedin.com -> linkedin.com\n",
    "        elif domain.endswith(\".linkedin.com\"):\n",
    "            cleaned_domains[i] = \"linkedin.com\"\n",
    "        # de.wikipedia.org -> wikipedia.org\n",
    "        elif domain.endswith(\".wikipedia.org\"):\n",
    "            cleaned_domains[i] = \"wikipedia.org\"\n",
    "        if domain.endswith(\"de.wordpress.org\"):\n",
    "            cleaned_domains[i] = \"wordpress.org\"\n",
    "\n",
    "\n",
    "    # Remove duplicates and sort\n",
    "    cleaned_links = sorted(set(cleaned_links))\n",
    "    cleaned_domains = sorted(set(cleaned_domains))\n",
    "    \n",
    "\n",
    "    # Construct the output dictionary entry for this row\n",
    "    entry = {\n",
    "        \"SKZ\": row['SKZ'],\n",
    "        \"base_url\": row['base_url'],\n",
    "        # \"stripped_base_url\": stripped_base_url,\n",
    "        # \"internal_links\": row['internal_links'],\n",
    "        # \"visited_internal_links\": row['visited_internal_links'],\n",
    "        # \"attempted_visits_links\": row['attempted_visits_links'],\n",
    "        # \"all_links\": row['all_links'],\n",
    "        \"cleaned_ext_links\": cleaned_links,\n",
    "        \"cleaned_ext_domains\": cleaned_domains,\n",
    "        \"count_ext_links\": len(cleaned_links),\n",
    "        \"count_ext_domains\": len(cleaned_domains),\n",
    "        \"count_int_links\": len(row['internal_links']),\n",
    "        \"count_attempted_links\": len(row['attempted_visits_links']),\n",
    "        \"count_visited_links\": len(row['visited_internal_links']),\n",
    "    }\n",
    "    \n",
    "    output_data.append(entry)\n",
    "\n",
    "# save the cleaned links to a new json file with a filename that's based on the original path\n",
    "path_cleaned = path.replace(\".json\", \"_cleaned.json\")\n",
    "with open(path_cleaned, 'w') as f:\n",
    "    json.dump(output_data, f, indent=4)\n",
    "print(f\"Cleaned links saved to {path_cleaned}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775d57a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descriptive stats of internal links by school_type_mapping, without schools that have 0 links. \n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "# Load the school type CSV file\n",
    "csv_path = './comparison_df_2024-08-12.csv'\n",
    "comparison_df = pd.read_csv(csv_path)\n",
    "\n",
    "# Load the scraped links JSON file\n",
    "file_path = 'scraped_links_2024-08-12_cleaned.json'\n",
    "with open(file_path, 'r') as file:\n",
    "    schools_data = json.load(file)\n",
    "\n",
    "# Convert JSON data to DataFrame\n",
    "links_df = pd.DataFrame(schools_data)\n",
    "\n",
    "# Merge the two DataFrames on 'SKZ'\n",
    "merged_df = comparison_df.merge(links_df, on='SKZ', how='left')\n",
    "\n",
    "# Filter out schools with count_int_links == 0\n",
    "filtered_df = merged_df[merged_df['count_int_links'] > 0]\n",
    "\n",
    "# Group by 'school_type_mapping' and calculate statistics for 'count_int_links'\n",
    "statistics_df = filtered_df.groupby('school_type_mapping').agg({\n",
    "    'count_int_links': ['mean', 'std', 'median', 'min', 'max', 'count']\n",
    "}).reset_index()\n",
    "\n",
    "# Flatten multi-level columns\n",
    "statistics_df.columns = ['School Type Mapping', 'Mean', 'Std Dev', 'Median', 'Min', 'Max', 'N']\n",
    "\n",
    "# Calculate overall statistics for all schools with non-zero internal links\n",
    "overall_stats = filtered_df['count_int_links'].agg(['mean', 'std', 'median', 'min', 'max', 'count'])\n",
    "overall_row = pd.DataFrame([[\n",
    "    'Total',\n",
    "    overall_stats['count'],\n",
    "    overall_stats['mean'],\n",
    "    overall_stats['std'],\n",
    "    overall_stats['median'],\n",
    "    overall_stats['min'],\n",
    "    overall_stats['max']\n",
    "]], columns=['School Type Mapping', 'N', 'Mean', 'Std Dev', 'Median', 'Min', 'Max'])\n",
    "\n",
    "# Append the 'Total' row to the statistics DataFrame\n",
    "statistics_df = pd.concat([statistics_df, overall_row], ignore_index=True)\n",
    "\n",
    "# Reorder columns\n",
    "statistics_df = statistics_df[['School Type Mapping', 'N', 'Mean', 'Std Dev', 'Median', 'Min', 'Max']]\n",
    "\n",
    "# Print LaTeX table code\n",
    "latex_code = '''\n",
    "\\\\begin{table}[htbp]\n",
    "\\\\centering\n",
    "\\\\resizebox{\\\\textwidth}{!}{\n",
    "    \\\\begin{tabular}{lcccccc}\n",
    "    \\\\toprule\n",
    "    School Type Mapping & N & Mean & Std Dev & Median & Min & Max \\\\\\\\\n",
    "    \\\\midrule\n",
    "'''\n",
    "\n",
    "for index, row in statistics_df.iterrows():\n",
    "    latex_code += f\"    {row['School Type Mapping']} & {int(row['N'])} & {row['Mean']:.2f} & {row['Std Dev']:.2f} & {row['Median']:.2f} & {row['Min']:.2f} & {row['Max']:.2f} \\\\\\\\\\n\"\n",
    "\n",
    "latex_code += '''\n",
    "    \\\\bottomrule\n",
    "    \\\\end{tabular}\n",
    "}\n",
    "\\\\caption{Statistics of Internal Links by School Type Mapping}\n",
    "\\\\label{tab:internal_links_stats_mapping}\n",
    "\\\\end{table}\n",
    "'''\n",
    "\n",
    "print(latex_code)\n",
    "#  Don't forget to add a \\midrule before the total row. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea208444",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison of Internal Sites Discovered, Attempted, and Visited by School Type Mapping\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "# Load the school type CSV file\n",
    "csv_path = './comparison_df_2024-08-12.csv'\n",
    "comparison_df = pd.read_csv(csv_path)\n",
    "\n",
    "# Load the scraped links JSON file\n",
    "file_path = 'scraped_links_2024-08-12_cleaned.json'\n",
    "with open(file_path, 'r') as file:\n",
    "    schools_data = json.load(file)\n",
    "\n",
    "# Convert JSON data to DataFrame\n",
    "links_df = pd.DataFrame(schools_data)\n",
    "\n",
    "# Merge the two DataFrames on 'SKZ'\n",
    "merged_df = comparison_df.merge(links_df, on='SKZ', how='left')\n",
    "\n",
    "# Filter out schools with count_int_links == 0\n",
    "filtered_df = merged_df[merged_df['count_int_links'] > 0]\n",
    "\n",
    "# Group by 'school_type_mapping' and calculate statistics for 'count_int_links', 'count_attempted_links', and 'count_visited_links'\n",
    "statistics_df = filtered_df.groupby('school_type_mapping').agg({\n",
    "    'count_int_links': ['mean', 'std', 'count'],\n",
    "    'count_attempted_links': ['mean', 'std'],\n",
    "    'count_visited_links': ['mean', 'std']\n",
    "}).reset_index()\n",
    "\n",
    "# Flatten multi-level columns\n",
    "statistics_df.columns = ['School Type Mapping', 'N_Discovered_Mean', 'N_Discovered_SD', 'N_Discovered_Count',\n",
    "                          'Attempted_Mean', 'Attempted_SD', 'Visited_Mean', 'Visited_SD']\n",
    "\n",
    "# Calculate overall statistics for all schools with non-zero internal links\n",
    "overall_stats = filtered_df.agg({\n",
    "    'count_int_links': ['mean', 'std', 'count'],\n",
    "    'count_attempted_links': ['mean', 'std'],\n",
    "    'count_visited_links': ['mean', 'std']\n",
    "})\n",
    "overall_row = pd.DataFrame([[\n",
    "    'Total',\n",
    "    overall_stats['count_int_links']['mean'],\n",
    "    overall_stats['count_int_links']['std'],\n",
    "    overall_stats['count_int_links']['count'],\n",
    "    overall_stats['count_attempted_links']['mean'],\n",
    "    overall_stats['count_attempted_links']['std'],\n",
    "    overall_stats['count_visited_links']['mean'],\n",
    "    overall_stats['count_visited_links']['std']\n",
    "]], columns=['School Type Mapping', 'N_Discovered_Mean', 'N_Discovered_SD', 'N_Discovered_Count',\n",
    "             'Attempted_Mean', 'Attempted_SD', 'Visited_Mean', 'Visited_SD'])\n",
    "\n",
    "# Append the 'Total' row to the statistics DataFrame\n",
    "statistics_df = pd.concat([statistics_df, overall_row], ignore_index=True)\n",
    "\n",
    "# Calculate the total numbers of discovered, attempted, and visited links\n",
    "total_N = int(merged_df['count_int_links'].count())\n",
    "total_discovered = int(merged_df['count_int_links'].sum())\n",
    "total_attempted = int(merged_df['count_attempted_links'].sum())\n",
    "total_visited = int(merged_df['count_visited_links'].sum())\n",
    "\n",
    "# Print LaTeX table code\n",
    "latex_code = '''\n",
    "\\\\begin{table}[htbp]\n",
    "\\\\centering\n",
    "\\\\resizebox{\\\\textwidth}{!}{\n",
    "    \\\\begin{tabular}{lccc*{2}{cc}*{2}{cc}}\n",
    "    \\\\toprule\n",
    "    \\\\multirow{2}{*}{\\\\makecell[l]{School Type}} & \\\\multirow{2}{*}{n} & \\\\multicolumn{2}{c}{Internal} & \\\\multicolumn{2}{c}{Attempted} & \\\\multicolumn{2}{c}{Visited} \\\\\\\\\n",
    "    \\\\cmidrule(lr){3-4} \\\\cmidrule(lr){5-6} \\\\cmidrule(lr){7-8}\n",
    "    & & Avg. & SD & Avg. & SD & Avg. & SD \\\\\\\\\n",
    "    \\\\midrule\n",
    "'''\n",
    "\n",
    "for index, row in statistics_df.iterrows():\n",
    "    latex_code += f\"    {row['School Type Mapping']} & {int(row['N_Discovered_Count'])} & {row['N_Discovered_Mean']:.2f} & {row['N_Discovered_SD']:.2f} & {row['Attempted_Mean']:.2f} & {row['Attempted_SD']:.2f} & {row['Visited_Mean']:.2f} & {row['Visited_SD']:.2f} \\\\\\\\\\n\"\n",
    "\n",
    "latex_code += '''\n",
    "    \\\\midrule\n",
    "    Absolute n of webpages discovered / visited & {} & \\\\multicolumn{{2}}{{c}}{{{:,}}} & \\\\multicolumn{{2}}{{c}}{{{:,}}} & \\\\multicolumn{{2}}{{c}}{{{:,}}} \\\\\\\\\n",
    "'''.format(total_N, total_discovered, total_attempted, total_visited)\n",
    "\n",
    "latex_code += '''\n",
    "    \\\\bottomrule\n",
    "    \\\\end{tabular}\n",
    "}\n",
    "\\\\caption{Website Visits Compared: Internal Links Discovered, Attempted to Visit, and Actually Visited}\n",
    "\\\\label{tab:sites_comparison}\n",
    "\\\\end{table}\n",
    "'''\n",
    "\n",
    "print(latex_code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213c97b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# External links with domains by school_type_mapping, without schools that have 0 links.\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "# Load the school type CSV file\n",
    "csv_path = './comparison_df_2024-08-12.csv'\n",
    "comparison_df = pd.read_csv(csv_path)\n",
    "\n",
    "# Load the scraped links JSON file\n",
    "file_path = 'scraped_links_2024-08-12_cleaned.json'\n",
    "with open(file_path, 'r') as file:\n",
    "    schools_data = json.load(file)\n",
    "\n",
    "# Convert JSON data to DataFrame\n",
    "links_df = pd.DataFrame(schools_data)\n",
    "\n",
    "# Merge the two DataFrames on 'SKZ'\n",
    "merged_df = comparison_df.merge(links_df, on='SKZ', how='left')\n",
    "\n",
    "# Filter out schools with count_int_links == 0\n",
    "filtered_df = merged_df[merged_df['count_int_links'] > 0]\n",
    "\n",
    "# Group by 'school_type_mapping' and calculate statistics for 'count_ext_links' and 'count_ext_domains'\n",
    "statistics_df = filtered_df.groupby('school_type_mapping').agg({\n",
    "    'count_ext_links': ['mean', 'std', 'median', 'count'],\n",
    "    'count_ext_domains': ['mean', 'std', 'median']\n",
    "}).reset_index()\n",
    "\n",
    "# Flatten multi-level columns\n",
    "statistics_df.columns = ['School Type Mapping', 'Ext. Links Mean', 'Ext. Links SD', 'Ext. Links Median', 'N', 'Ext. Domains Mean', 'Ext. Domains SD', 'Ext. Domains Median']\n",
    "\n",
    "# Calculate overall statistics for all schools with non-zero internal links\n",
    "overall_stats = filtered_df.agg({\n",
    "    'count_ext_links': ['mean', 'std', 'median', 'count'],\n",
    "    'count_ext_domains': ['mean', 'std', 'median']\n",
    "})\n",
    "overall_row = pd.DataFrame([[\n",
    "    'Total',\n",
    "    overall_stats['count_ext_links']['mean'],\n",
    "    overall_stats['count_ext_links']['std'],\n",
    "    overall_stats['count_ext_links']['median'],\n",
    "    overall_stats['count_ext_links']['count'],\n",
    "    overall_stats['count_ext_domains']['mean'],\n",
    "    overall_stats['count_ext_domains']['std'],\n",
    "    overall_stats['count_ext_domains']['median']\n",
    "]], columns=['School Type Mapping', 'Ext. Links Mean', 'Ext. Links SD', 'Ext. Links Median', 'N', 'Ext. Domains Mean', 'Ext. Domains SD', 'Ext. Domains Median'])\n",
    "\n",
    "# Append the 'Total' row to the statistics DataFrame\n",
    "statistics_df = pd.concat([statistics_df, overall_row], ignore_index=True)\n",
    "\n",
    "# Print LaTeX table code\n",
    "latex_code = '''\n",
    "\\\\begin{table}[htbp]\n",
    "\\\\centering\n",
    "\\\\resizebox{\\\\textwidth}{!}{\n",
    "    \\\\begin{tabular}{lccc*{3}{cc}}\n",
    "    \\\\toprule\n",
    "    \\\\multirow{2}{*}{\\\\makecell[l]{School Type}} & \\\\multirow{2}{*}{n} & \\\\multicolumn{3}{c}{Ext. Links} & \\\\multicolumn{3}{c}{Ext. Domains} \\\\\\\\\n",
    "    \\\\cmidrule(lr){3-5} \\\\cmidrule(lr){6-8}\n",
    "    & & Avg. & SD & Median & Avg. & SD & Median \\\\\\\\\n",
    "    \\\\midrule\n",
    "'''\n",
    "\n",
    "for index, row in statistics_df.iterrows():\n",
    "    latex_code += f\"    {row['School Type Mapping']} & {int(row['N'])} & {row['Ext. Links Mean']:.2f} & {row['Ext. Links SD']:.2f} & {row['Ext. Links Median']:.2f} & {row['Ext. Domains Mean']:.2f} & {row['Ext. Domains SD']:.2f} & {row['Ext. Domains Median']:.2f} \\\\\\\\\\n\"\n",
    "\n",
    "latex_code += '''\n",
    "    \\\\bottomrule\n",
    "    \\\\end{tabular}\n",
    "}\n",
    "\\\\caption{Comparison of External Links and Domains by School Type Mapping}\n",
    "\\\\label{tab:external_links_domains}\n",
    "\\\\end{table}\n",
    "'''\n",
    "\n",
    "print(latex_code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b1fb2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# External links with domains by school_type_text, with median and total, without schools that have 0 links.\n",
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "# Load the school type CSV file\n",
    "csv_path = './comparison_df_2024-08-12.csv'\n",
    "comparison_df = pd.read_csv(csv_path)\n",
    "\n",
    "# Load the scraped links JSON file\n",
    "file_path = 'scraped_links_2024-08-12_cleaned.json'\n",
    "with open(file_path, 'r') as file:\n",
    "    schools_data = json.load(file)\n",
    "\n",
    "# Convert JSON data to DataFrame\n",
    "links_df = pd.DataFrame(schools_data)\n",
    "\n",
    "# Merge the two DataFrames on 'SKZ'\n",
    "merged_df = comparison_df.merge(links_df, on='SKZ', how='left')\n",
    "\n",
    "# Filter out schools with count_int_links == 0\n",
    "filtered_df = merged_df[merged_df['count_int_links'] > 0]\n",
    "\n",
    "# Group by 'school_type_txt' and calculate statistics for 'count_ext_links' and 'count_ext_domains'\n",
    "statistics_df = filtered_df.groupby('school_type_txt').agg({\n",
    "    'count_ext_links': ['mean', 'std', 'median', 'count'],\n",
    "    'count_ext_domains': ['mean', 'std', 'median']\n",
    "}).reset_index()\n",
    "\n",
    "# Flatten multi-level columns\n",
    "statistics_df.columns = ['School Type', 'Ext. Links Mean', 'Ext. Links SD', 'Ext. Links Median', 'N', 'Ext. Domains Mean', 'Ext. Domains SD', 'Ext. Domains Median']\n",
    "\n",
    "# Calculate overall statistics for all schools with non-zero internal links\n",
    "overall_stats = filtered_df.agg({\n",
    "    'count_ext_links': ['mean', 'std', 'median', 'count'],\n",
    "    'count_ext_domains': ['mean', 'std', 'median']\n",
    "})\n",
    "overall_row = pd.DataFrame([[\n",
    "    'Total',\n",
    "    overall_stats['count_ext_links']['mean'],\n",
    "    overall_stats['count_ext_links']['std'],\n",
    "    overall_stats['count_ext_links']['median'],\n",
    "    overall_stats['count_ext_links']['count'],\n",
    "    overall_stats['count_ext_domains']['mean'],\n",
    "    overall_stats['count_ext_domains']['std'],\n",
    "    overall_stats['count_ext_domains']['median']\n",
    "]], columns=['School Type', 'Ext. Links Mean', 'Ext. Links SD', 'Ext. Links Median', 'N', 'Ext. Domains Mean', 'Ext. Domains SD', 'Ext. Domains Median'])\n",
    "\n",
    "# Append the 'Total' row to the statistics DataFrame\n",
    "statistics_df = pd.concat([statistics_df, overall_row], ignore_index=True)\n",
    "\n",
    "# Print LaTeX table code\n",
    "latex_code = '''\n",
    "\\\\begin{table}[htbp]\n",
    "\\\\centering\n",
    "\\\\resizebox{\\\\textwidth}{!}{\n",
    "    \\\\begin{tabular}{lccc*{3}{cc}}\n",
    "    \\\\toprule\n",
    "    \\\\multirow{2}{*}{\\\\makecell[l]{School Type}} & \\\\multirow{2}{*}{n} & \\\\multicolumn{3}{c}{Ext. Links} & \\\\multicolumn{3}{c}{Ext. Domains} \\\\\\\\\n",
    "    \\\\cmidrule(lr){3-5} \\\\cmidrule(lr){6-8}\n",
    "    & & Avg. & SD & Median & Avg. & SD & Median \\\\\\\\\n",
    "    \\\\midrule\n",
    "'''\n",
    "\n",
    "for index, row in statistics_df.iterrows():\n",
    "    latex_code += f\"    {row['School Type']} & {int(row['N'])} & {row['Ext. Links Mean']:.2f} & {row['Ext. Links SD']:.2f} & {row['Ext. Links Median']:.2f} & {row['Ext. Domains Mean']:.2f} & {row['Ext. Domains SD']:.2f} & {row['Ext. Domains Median']:.2f} \\\\\\\\\\n\"\n",
    "\n",
    "latex_code += '''\n",
    "    \\\\bottomrule\n",
    "    \\\\end{tabular}\n",
    "}\n",
    "\\\\caption{Comparison of External Links and Domains by School Type}\n",
    "\\\\label{tab:external_links_domains_txt}\n",
    "\\\\end{table}\n",
    "'''\n",
    "\n",
    "print(latex_code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e719f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison of External Links and Domains by School Type (needs manual adding of two \\ to )\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "# Load the school type CSV file\n",
    "csv_path = './comparison_df_2024-08-12.csv'\n",
    "comparison_df = pd.read_csv(csv_path)\n",
    "\n",
    "# Load the scraped links JSON file\n",
    "file_path = 'scraped_links_2024-08-12_cleaned.json'\n",
    "with open(file_path, 'r') as file:\n",
    "    schools_data = json.load(file)\n",
    "\n",
    "# Convert JSON data to DataFrame\n",
    "links_df = pd.DataFrame(schools_data)\n",
    "\n",
    "# Merge the two DataFrames on 'SKZ'\n",
    "merged_df = comparison_df.merge(links_df, on='SKZ', how='left')\n",
    "\n",
    "# Filter out schools with count_int_links == 0\n",
    "filtered_df = merged_df[merged_df['count_int_links'] > 0]\n",
    "\n",
    "# Group by 'school_type_txt' and calculate statistics for 'count_ext_links' and 'count_ext_domains'\n",
    "statistics_df = filtered_df.groupby('school_type_txt').agg({\n",
    "    'count_ext_links': ['mean', 'std', 'count'],\n",
    "    'count_ext_domains': ['mean', 'std']\n",
    "}).reset_index()\n",
    "\n",
    "# Flatten multi-level columns\n",
    "statistics_df.columns = ['School Type', 'Ext. Links Mean', 'Ext. Links SD', 'N', 'Ext. Domains Mean', 'Ext. Domains SD']\n",
    "\n",
    "# Calculate overall statistics for all schools with non-zero internal links\n",
    "overall_stats = filtered_df.agg({\n",
    "    'count_ext_links': ['mean', 'std', 'count'],\n",
    "    'count_ext_domains': ['mean', 'std']\n",
    "})\n",
    "overall_row = pd.DataFrame([[\n",
    "    'Total',\n",
    "    overall_stats['count_ext_links']['mean'],\n",
    "    overall_stats['count_ext_links']['std'],\n",
    "    overall_stats['count_ext_links']['count'],\n",
    "    overall_stats['count_ext_domains']['mean'],\n",
    "    overall_stats['count_ext_domains']['std']\n",
    "]], columns=['School Type', 'Ext. Links Mean', 'Ext. Links SD', 'N', 'Ext. Domains Mean', 'Ext. Domains SD'])\n",
    "\n",
    "# Append the 'Total' row to the statistics DataFrame\n",
    "statistics_df = pd.concat([statistics_df, overall_row], ignore_index=True)\n",
    "\n",
    "# Print LaTeX table code\n",
    "latex_code = '''\n",
    "\\\\begin{table}[htbp]\n",
    "\\\\centering\n",
    "\\\\resizebox{\\\\textwidth}{!}{\n",
    "    \\\\begin{tabular}{lccc*{2}{cc}}\n",
    "    \\\\toprule\n",
    "    \\\\multirow{2}{*}{\\\\makecell[l]{School Type}} & \\\\multirow{2}{*}{n} & \\\\multicolumn{2}{c}{Ext. Links} & \\\\multicolumn{2}{c}{Ext. Domains} \\\\\n",
    "    \\\\cmidrule(lr){3-4} \\\\cmidrule(lr){5-6}\n",
    "    & & Avg. & SD & Avg. & SD \\\\\n",
    "    \\\\midrule\n",
    "'''\n",
    "\n",
    "for index, row in statistics_df.iterrows():\n",
    "    latex_code += f\"    {row['School Type']} & {int(row['N'])} & {row['Ext. Links Mean']:.2f} & {row['Ext. Links SD']:.2f} & {row['Ext. Domains Mean']:.2f} & {row['Ext. Domains SD']:.2f} \\\\\\\\\\n\"\n",
    "\n",
    "latex_code += '''\n",
    "    \\\\bottomrule\n",
    "    \\\\end{tabular}\n",
    "}\n",
    "\\\\caption{Comparison of External Links and Domains by School Type}\n",
    "\\\\label{tab:external_links_domains_txt_no_median}\n",
    "\\\\end{table}\n",
    "'''\n",
    "\n",
    "print(latex_code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1aea240",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "# Load the comparison dataframe\n",
    "csv_path = './comparison_df_2024-08-12.csv'\n",
    "comparison_df = pd.read_csv(csv_path)\n",
    "\n",
    "# Load the JSON file with scraped links data\n",
    "file_path = 'scraped_links_2024-08-12_cleaned.json'\n",
    "with open(file_path, 'r') as file:\n",
    "    schools_data = json.load(file)\n",
    "\n",
    "# Convert the JSON data to a dataframe\n",
    "scraped_df = pd.DataFrame(schools_data)\n",
    "\n",
    "# Merge the comparison dataframe with scraped data based on 'SKZ'\n",
    "merged_df = pd.merge(comparison_df[['SKZ', 'school_type_mapping']], scraped_df, on='SKZ')\n",
    "\n",
    "# Group by 'school_type_mapping' and calculate the statistics\n",
    "grouped_stats = merged_df.groupby('school_type_mapping').agg(\n",
    "    avg_int_links=('count_int_links', 'mean'),\n",
    "    std_int_links=('count_int_links', 'std'),\n",
    "    median_int_links=('count_int_links', 'median'),\n",
    "    min_int_links=('count_int_links', 'min'),\n",
    "    max_int_links=('count_int_links', 'max'),\n",
    "    avg_attempted_links=('count_attempted_links', 'mean'),\n",
    "    std_attempted_links=('count_attempted_links', 'std'),\n",
    "    avg_visited_links=('count_visited_links', 'mean'),\n",
    "    std_visited_links=('count_visited_links', 'std')\n",
    ").reset_index()\n",
    "\n",
    "# Generate LaTeX table for internal links statistics\n",
    "latex_table_1 = grouped_stats[['school_type_mapping', 'avg_int_links', 'std_int_links', 'median_int_links', \n",
    "                               'min_int_links', 'max_int_links']].to_latex(index=False, \n",
    "                               header=[\"School Type\", \"Average\", \"Standard Deviation\", \"Median\", \"Min\", \"Max\"], \n",
    "                               column_format=\"lccccc\", \n",
    "                               float_format=\"%.2f\", \n",
    "                               caption=\"Internal Links Statistics by School Type\", \n",
    "                               label=\"tab:internal_links_stats\")\n",
    "\n",
    "# Generate LaTeX table comparing internal, attempted, and visited links\n",
    "latex_table_2 = grouped_stats[['school_type_mapping', 'avg_int_links', 'std_int_links', \n",
    "                               'avg_attempted_links', 'std_attempted_links', \n",
    "                               'avg_visited_links', 'std_visited_links']].to_latex(index=False, \n",
    "                               header=[\"School Type\", \"Internal Links\", \"\", \"Attempted Links\", \"\", \"Visited Links\", \"\"], \n",
    "                               column_format=\"lcccccc\", \n",
    "                               float_format=\"%.2f\", \n",
    "                               multicolumn=True, \n",
    "                               multicolumn_format=\"c\", \n",
    "                               caption=\"Comparison of Internal, Attempted, and Visited Links by School Type\", \n",
    "                               label=\"tab:link_comparison\")\n",
    "\n",
    "print(latex_table_1)\n",
    "print(latex_table_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a22237",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import Counter\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "# Step 1: Load the JSON file\n",
    "file_path = 'scraped_links_2024-08-12_cleaned.json'  # Replace with the path to your JSON file\n",
    "with open(file_path, 'r') as file:\n",
    "    schools_data = json.load(file)\n",
    "\n",
    "# Step 2: Extract the cleaned_domains\n",
    "cleaned_domains = []\n",
    "for school in schools_data:\n",
    "    cleaned_domains.extend(school.get('cleaned_ext_domains', []))\n",
    "\n",
    "# Step 3: Extract the TLD from each domain\n",
    "tlds = []\n",
    "for domain in cleaned_domains:\n",
    "    parsed_url = urlparse(f'http://{domain}')  # Ensure URL is parsed correctly\n",
    "    tld = parsed_url.hostname.split('.')[-1]  # Extract TLD\n",
    "    tlds.append(tld)\n",
    "\n",
    "# Step 4: Count the frequencies of each TLD\n",
    "tld_counts = Counter(tlds)\n",
    "\n",
    "# Step 5: Sort TLDs by the length of the TLD string\n",
    "sorted_tlds_by_length = sorted(tld_counts.items(), key=lambda item: len(item[0]))\n",
    "\n",
    "# Display the result\n",
    "for tld, count in sorted_tlds_by_length:\n",
    "    print(f\"{tld}: {count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db7d395",
   "metadata": {},
   "source": [
    "# 5. Categorization and Summarization\n",
    "This scraper will categorize all external domains. \n",
    "\n",
    "For this purpose it will go through several steps: \n",
    "1. Visit each domain for the first 400 characters\n",
    "2. It will use a GPT model to summarize what they are doing\n",
    "3. Based on the summary it will categorize the organization based on 13 pre-defined categories\n",
    "\n",
    "In case a website cannot be reached, a follow-up code block can re-attempt only those actors that are missing in the previous output file. \n",
    "\n",
    "Summaries are done in German and English.\n",
    "\n",
    "Possible improvements: \n",
    "- Clearer Separation of steps, currently several approaches were compared and tested and old artifacts are still present. \n",
    "- Simpler testing for clarity. Manual checks for obvious miscategorisations were necessary because sometimes multiple categories may fit. \n",
    "- Clearer iterative code for those website not reached yet. First try all, then try those missing\n",
    "\n",
    "Main learnings: \n",
    "- When using sample input and output, the categorization will be biased towards the provided sample categories. Therefore it is better to just give more generic prompts and no eample output data. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f99b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "import aiohttp\n",
    "import asyncio\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urlparse\n",
    "import time\n",
    "\n",
    "# Apply the nest_asyncio patch\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Function to format URLs\n",
    "def format_url(url):\n",
    "    if not url.startswith(('http://', 'https://')):\n",
    "        url = 'http://' + url\n",
    "    return url\n",
    "\n",
    "# Function to clean text\n",
    "def clean_text(text):\n",
    "    return ' '.join(text.split())\n",
    "\n",
    "# Function to try different URL formats\n",
    "async def try_different_formats(session, url, headers):\n",
    "    formats = [\n",
    "        url,\n",
    "        url.replace('http://', 'https://'),\n",
    "        'http://www.' + urlparse(url).netloc,\n",
    "        'https://www.' + urlparse(url).netloc\n",
    "    ]\n",
    "    for fmt in formats:\n",
    "        try:\n",
    "            async with session.get(fmt, headers=headers, timeout=10) as response:\n",
    "                if response.status == 200:\n",
    "                    html = await response.text()\n",
    "                    soup = BeautifulSoup(html, 'html.parser')\n",
    "                    text = soup.get_text()\n",
    "                    cleaned_text = clean_text(text[:600])  # Clean the text\n",
    "                    return fmt, cleaned_text\n",
    "        except Exception:\n",
    "            time.sleep(1)  # Introduce a delay before retrying\n",
    "            continue\n",
    "    return url, \"Connection failed: All formats tried\"\n",
    "\n",
    "# Function to fetch and parse a single URL\n",
    "async def fetch(session, url):\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/93.0.4577.82 Safari/537.36\"\n",
    "    }\n",
    "    url = format_url(url)  # Format the URL\n",
    "    try:\n",
    "        async with session.get(url, headers=headers, timeout=15) as response:\n",
    "            if response.status == 200:\n",
    "                html = await response.text()\n",
    "                soup = BeautifulSoup(html, 'html.parser')\n",
    "                text = soup.get_text()\n",
    "                cleaned_text = clean_text(text[:600])  # Clean the text\n",
    "                return url, cleaned_text  # Return the URL and the cleaned text\n",
    "            else:\n",
    "                return await try_different_formats(session, url, headers)\n",
    "    except aiohttp.ClientError:\n",
    "        return await try_different_formats(session, url, headers)\n",
    "    except Exception as e:\n",
    "        return url, f\"Connection failed: {str(e)}\"\n",
    "\n",
    "# Function to handle fetching multiple URLs\n",
    "async def fetch_all(urls):\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        tasks = []\n",
    "        for url in urls:\n",
    "            task = asyncio.ensure_future(fetch(session, url))\n",
    "            tasks.append(task)\n",
    "            await asyncio.sleep(0.1)  # Introduce a small delay after each request\n",
    "        responses = await asyncio.gather(*tasks)\n",
    "        return responses\n",
    "\n",
    "# List of 5000 URLs to visit\n",
    "urls = unique_domains\n",
    "\n",
    "# Run the scraper\n",
    "responses = asyncio.run(fetch_all(urls))\n",
    "\n",
    "# Convert results to a DataFrame\n",
    "df = pd.DataFrame(responses, columns=[\"Domain\", \"Text\"])\n",
    "\n",
    "# Evaluate the results\n",
    "successful_connections = df[df[\"Text\"].str.contains(\"Connection failed\") == False].shape[0]\n",
    "failed_connections = df[df[\"Text\"].str.contains(\"Connection failed\")].shape[0]\n",
    "\n",
    "print(f\"Successful connections: {successful_connections}\")\n",
    "print(f\"Failed connections: {failed_connections}\")\n",
    "\n",
    "# Display the first few rows of the dataframe\n",
    "print(df.head())\n",
    "\n",
    "# Save the results to a CSV file\n",
    "df.to_csv(\"5_scraped_data_round2.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e244306",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop those where the text has less than 80 characters\n",
    "df = df[df['Text'].str.len() > 80]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdccad1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.1.1 Try again with those where connection has failed. \n",
    "\n",
    "import nest_asyncio\n",
    "import aiohttp\n",
    "import asyncio\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urlparse\n",
    "import time\n",
    "\n",
    "# Apply the nest_asyncio patch\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Function to format URLs\n",
    "def format_url(url):\n",
    "    if not url.startswith(('http://', 'https://')):\n",
    "        url = 'http://' + url\n",
    "    return url\n",
    "\n",
    "# Function to clean text\n",
    "def clean_text(text):\n",
    "    return ' '.join(text.split())\n",
    "\n",
    "# Function to try different URL formats\n",
    "async def try_different_formats(session, url, headers):\n",
    "    formats = [\n",
    "        url,\n",
    "        url.replace('http://', 'https://'),\n",
    "        'http://www.' + urlparse(url).netloc,\n",
    "        'https://www.' + urlparse(url).netloc\n",
    "    ]\n",
    "    for fmt in formats:\n",
    "        try:\n",
    "            async with session.get(fmt, headers=headers, timeout=10) as response:\n",
    "                if response.status == 200:\n",
    "                    html = await response.text()\n",
    "                    soup = BeautifulSoup(html, 'html.parser')\n",
    "                    text = soup.get_text()\n",
    "                    cleaned_text = clean_text(text[:600])  # Clean the text\n",
    "                    return fmt, cleaned_text\n",
    "        except Exception:\n",
    "            time.sleep(1)  # Introduce a delay before retrying\n",
    "            continue\n",
    "    return url, \"Connection failed: All formats tried\"\n",
    "\n",
    "# Function to fetch and parse a single URL\n",
    "async def fetch(session, url):\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/93.0.4577.82 Safari/537.36\"\n",
    "    }\n",
    "    url = format_url(url)  # Format the URL\n",
    "    try:\n",
    "        async with session.get(url, headers=headers, timeout=10) as response:\n",
    "            if response.status == 200:\n",
    "                html = await response.text()\n",
    "                soup = BeautifulSoup(html, 'html.parser')\n",
    "                text = soup.get_text()\n",
    "                cleaned_text = clean_text(text[:600])  # Clean the text\n",
    "                return url, cleaned_text  # Return the URL and the cleaned text\n",
    "            else:\n",
    "                return await try_different_formats(session, url, headers)\n",
    "    except aiohttp.ClientError:\n",
    "        return await try_different_formats(session, url, headers)\n",
    "    except Exception as e:\n",
    "        return url, f\"Connection failed: {str(e)}\"\n",
    "\n",
    "# Function to handle fetching multiple URLs\n",
    "async def fetch_all(urls):\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        tasks = []\n",
    "        for url in urls:\n",
    "            task = asyncio.create_task(fetch(session, url))\n",
    "            tasks.append(task)\n",
    "            await asyncio.sleep(0.3)  # Introduce a small delay after each request\n",
    "        responses = await asyncio.gather(*tasks)\n",
    "        return responses\n",
    "\n",
    "# Load the previously saved DataFrame\n",
    "df = pd.read_csv(\"scraped_data.csv\")\n",
    "\n",
    "# Filter out the URLs that had connection failures\n",
    "failed_urls = df[df[\"Text\"].str.contains(\"Connection failed\", na=False)][\"Domain\"].tolist()\n",
    "\n",
    "# Retry the failed URLs\n",
    "if failed_urls:\n",
    "    retry_responses = asyncio.run(fetch_all(failed_urls))\n",
    "\n",
    "    # Convert retry results to a DataFrame\n",
    "    retry_df = pd.DataFrame(retry_responses, columns=[\"Domain\", \"Text\"])\n",
    "\n",
    "    # Update the original DataFrame with the retry results\n",
    "    for index, row in retry_df.iterrows():\n",
    "        df.loc[df[\"Domain\"] == row[\"Domain\"], \"Text\"] = row[\"Text\"]\n",
    "\n",
    "    # Save the updated results to a CSV file\n",
    "    df.to_csv(\"scraped_data_updated.csv\", index=False)\n",
    "\n",
    "    # Evaluate the results\n",
    "    successful_connections = df[df[\"Text\"].str.contains(\"Connection failed\", na=False) == False].shape[0]\n",
    "    failed_connections = df[df[\"Text\"].str.contains(\"Connection failed\", na=False)].shape[0]\n",
    "\n",
    "    print(f\"Successful connections after retry: {successful_connections}\")\n",
    "    print(f\"Failed connections after retry: {failed_connections}\")\n",
    "\n",
    "    # Display the first few rows of the updated dataframe\n",
    "    print(df.head())\n",
    "else:\n",
    "    print(\"No failed connections to retry.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e506c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load CSV\n",
    "# df = pd.read_csv(\"scraped_data_round2.csv\")\n",
    "\n",
    "# Filter out rows with \"Connection failed\"\n",
    "df_filtered = df[~df[\"Text\"].str.contains(\"Connection failed\", na=False)]\n",
    "\n",
    "# Remove the http:// from the domain\n",
    "df_filtered[\"Domain\"] = df_filtered[\"Domain\"].str.replace(\"http://\", \"\", regex=False)\n",
    "\n",
    "# Also remove the www. from the domain\n",
    "df_filtered[\"Domain\"] = df_filtered[\"Domain\"].str.replace(\"www.\", \"\", regex=False)\n",
    "\n",
    "# and remove the nan entries\n",
    "df_filtered = df_filtered.dropna()\n",
    "\n",
    "# Convert to dictionary\n",
    "df_dict = df_filtered.set_index(\"Domain\")[\"Text\"].to_dict()\n",
    "\n",
    "# Print the dictionary line by line\n",
    "print(\"Final dictionary:\")\n",
    "for key, value in df_dict.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "\n",
    "# Count the total number of entries\n",
    "total_entries = len(df_dict)\n",
    "print(f\"\\nTotal number of entries: {total_entries}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217accb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.2 Automatic Summary of the text for each domain with GPT-4o-mini\n",
    "## Q: Which language to use for the summary? Original German or English? Or both?\n",
    "summary_prompt = \"\"\"\n",
    "You are a summariser of websites and only output JSON You have been provided with a list of URLs and the first 600 characters of text from each URL.\n",
    "- Please provide two summariesa summary of the text for each URL, one in German and one in English.\n",
    "- The summary should be concise and informative about the type of organization and what activities they might provide to schools and teaches.\n",
    "- You can also use what you already know about the organization from the URL.\n",
    "- Don't use any PII in the summaries.\n",
    "- If there is no text available, please leave it empty or provide a minimal summary if you know something about the URL.\"\"\"\n",
    "\n",
    "sum_input_example = \"\"\"{{\"example.com\": 'This is the website text from example.com', 'example2.com': 'This is the website text from example2.com'}\"\"\"\n",
    "sum_output_example = \"\"\"{\"example.com\": {\n",
    "   \"summary_en\": \"English Summary\",\n",
    "   \"summary_de\": \"German Summary\" \n",
    "  },\n",
    "  \"example2.com\": {\n",
    "   \"summary_en\": \"English Summary\",\n",
    "   \"summary_de\": \"German Summary\"\n",
    "  }\n",
    "}\"\"\"\n",
    "\n",
    "# # convert the first 5 entries from df_dict into a string\n",
    "# df_dict_5 = {k: v for k, v in list(df_dict.items())[:500]}\n",
    "# df_dict_5_str = json.dumps(df_dict_5)\n",
    "# df_dict_str = json.dumps(df_dict)\n",
    "\n",
    "import json\n",
    "import os\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "api_legacy_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "# Initialize OpenAI client\n",
    "client = OpenAI(api_key=api_legacy_key)\n",
    "\n",
    "# Function to process a batch of data\n",
    "def process_batch(batch):\n",
    "    try:\n",
    "        # Convert batch to JSON string\n",
    "        batch_str = json.dumps(batch)\n",
    "\n",
    "        # Make API request\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": summary_prompt},\n",
    "                {\"role\": \"user\", \"content\": sum_input_example},\n",
    "                {\"role\": \"assistant\", \"content\": sum_output_example},\n",
    "                {\"role\": \"user\", \"content\": batch_str}\n",
    "            ],\n",
    "            response_format={\"type\": \"json_object\"},\n",
    "            temperature=0.2,\n",
    "        )\n",
    "\n",
    "        # Return the entire message content\n",
    "        return response.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing batch: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "# Function to divide dictionary into batches\n",
    "def divide_into_batches(data_dict, batch_size):\n",
    "    items = list(data_dict.items())\n",
    "    for i in range(0, len(items), batch_size):\n",
    "        yield dict(items[i:i + batch_size])\n",
    "\n",
    "# Main processing\n",
    "batch_size = 100\n",
    "df_dict_batches = divide_into_batches(df_dict, batch_size)\n",
    "\n",
    "# Initialize a text string to store results\n",
    "results_text = \"\"\n",
    "\n",
    "# Initialize a counter for batch number\n",
    "batch_number = 1\n",
    "# Process each batch and append results to the text string\n",
    "for batch in df_dict_batches:\n",
    "    print(f\"Processing batch {batch_number}\")\n",
    "    batch_result = process_batch(batch)\n",
    "    results_text += batch_result + \"\\n\"\n",
    "    batch_number += 1\n",
    "\n",
    "\n",
    "# Save the results to a text file\n",
    "with open(\"site_summary_results_round2.txt\", \"w\") as file:\n",
    "    file.write(results_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7630bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def read_json_objects(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "    \n",
    "    json_objects = []\n",
    "    current_object = []\n",
    "    open_braces = 0\n",
    "    \n",
    "    for line in lines:\n",
    "        stripped_line = line.strip()\n",
    "        current_object.append(line)\n",
    "        open_braces += stripped_line.count('{')\n",
    "        open_braces -= stripped_line.count('}')\n",
    "        \n",
    "        if open_braces == 0 and current_object:\n",
    "            json_str = ''.join(current_object).strip()\n",
    "            if json_str.endswith(','):\n",
    "                json_str = json_str[:-1]\n",
    "            try:\n",
    "                json_obj = json.loads(json_str)\n",
    "                json_objects.append(json_obj)\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Error decoding JSON object: {e}\")\n",
    "                print(json_str)\n",
    "            current_object = []\n",
    "    \n",
    "    print(f\"Total JSON objects read: {len(json_objects)}\")  # Debugging statement\n",
    "    return json_objects\n",
    "\n",
    "def process_json_objects(json_objects):\n",
    "    standardized_data = {}\n",
    "    \n",
    "    for obj in json_objects:\n",
    "        if \"summaries\" in obj:\n",
    "            for summary in obj[\"summaries\"]:\n",
    "                standardized_data[summary[\"domain\"]] = {\n",
    "                    \"summary_en\": summary[\"summary_en\"],\n",
    "                    \"summary_de\": summary[\"summary_de\"]\n",
    "                }\n",
    "        else:\n",
    "            for domain, summaries in obj.items():\n",
    "                standardized_data[domain] = {\n",
    "                    \"summary_en\": summaries[\"summary_en\"],\n",
    "                    \"summary_de\": summaries[\"summary_de\"]\n",
    "                }\n",
    "    \n",
    "    print(f\"Total entries after processing: {len(standardized_data)}\")  # Debugging statement\n",
    "    return standardized_data\n",
    "\n",
    "def main(file_path):\n",
    "    json_objects = read_json_objects(file_path)\n",
    "    standardized_data = process_json_objects(json_objects)\n",
    "    \n",
    "    with open('standardized_site_summaries_round2.json', 'w') as file:\n",
    "        json.dump(standardized_data, file, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    print(f\"Final total entries written: {len(standardized_data)}\")  # Debugging statement\n",
    "\n",
    "# Path to the faulty JSON file\n",
    "file_path = 'site_summary_results_round2.txt'\n",
    "\n",
    "# Run the main function\n",
    "main(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c211c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove entries from standardized_site_summaries.json where summary_en is empty\n",
    "with open('standardized_site_summaries.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "data_filtered = {k: v for k, v in data.items() if v[\"summary_en\"] != \"\"} \n",
    "\n",
    "with open('standardized_site_summaries_filtered.json', 'w') as file:\n",
    "    json.dump(data_filtered, file, indent=2, ensure_ascii=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b3db6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find which entries don't have a summary\n",
    "# load the standardized_site_summaries.json\n",
    "with open('standardized_site_summaries.json', 'r') as file:\n",
    "    standardized_data = json.load(file)\n",
    "\n",
    "# check unique_domains and remove those items that have a summary in standardized_data\n",
    "unique_domains_no_summary = [domain for domain in unique_domains if domain not in standardized_data.keys()]\n",
    "print(f\"Unique domains without summary: {len(unique_domains_no_summary)}\")\n",
    "\n",
    "# add to unique_domains_no_summary those domains from standardized_data that have an empty summary\n",
    "unique_domains_no_summary += [domain for domain, summary in standardized_data.items() if summary[\"summary_en\"] == \"\"]\n",
    "\n",
    "# print length of unique_domains_no_summary\n",
    "print(f\"Unique domains without summary: {len(unique_domains_no_summary)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde87416",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "import aiohttp\n",
    "import asyncio\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urlparse\n",
    "import time\n",
    "\n",
    "# Apply the nest_asyncio patch\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Function to format URLs\n",
    "def format_url(url):\n",
    "    if not url.startswith(('http://', 'https://')):\n",
    "        url = 'http://' + url\n",
    "    return url\n",
    "\n",
    "# Function to clean text\n",
    "def clean_text(text):\n",
    "    return ' '.join(text.split())\n",
    "\n",
    "# Function to try different URL formats\n",
    "async def try_different_formats(session, url, headers):\n",
    "    formats = [\n",
    "        url,\n",
    "        url.replace('http://', 'https://'),\n",
    "        'http://www.' + urlparse(url).netloc,\n",
    "        'https://www.' + urlparse(url).netloc\n",
    "    ]\n",
    "    for fmt in formats:\n",
    "        try:\n",
    "            async with session.get(fmt, headers=headers, timeout=10) as response:\n",
    "                if response.status == 200:\n",
    "                    html = await response.text()\n",
    "                    soup = BeautifulSoup(html, 'html.parser')\n",
    "                    text = soup.get_text()\n",
    "                    cleaned_text = clean_text(text[:600])  # Clean the text\n",
    "                    return fmt, cleaned_text\n",
    "        except Exception:\n",
    "            time.sleep(1)  # Introduce a delay before retrying\n",
    "            continue\n",
    "    return url, \"Connection failed: All formats tried\"\n",
    "\n",
    "# Function to fetch and parse a single URL\n",
    "async def fetch(session, url):\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/93.0.4577.82 Safari/537.36\"\n",
    "    }\n",
    "    url = format_url(url)  # Format the URL\n",
    "    try:\n",
    "        async with session.get(url, headers=headers, timeout=15) as response:\n",
    "            if response.status == 200:\n",
    "                html = await response.text()\n",
    "                soup = BeautifulSoup(html, 'html.parser')\n",
    "                text = soup.get_text()\n",
    "                cleaned_text = clean_text(text[:600])  # Clean the text\n",
    "                return url, cleaned_text  # Return the URL and the cleaned text\n",
    "            else:\n",
    "                return await try_different_formats(session, url, headers)\n",
    "    except aiohttp.ClientError:\n",
    "        return await try_different_formats(session, url, headers)\n",
    "    except Exception as e:\n",
    "        return url, f\"Connection failed: {str(e)}\"\n",
    "\n",
    "# Function to handle fetching multiple URLs\n",
    "async def fetch_all(urls):\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        tasks = []\n",
    "        for url in urls:\n",
    "            task = asyncio.ensure_future(fetch(session, url))\n",
    "            tasks.append(task)\n",
    "            await asyncio.sleep(0.1)  # Introduce a small delay after each request\n",
    "        responses = await asyncio.gather(*tasks)\n",
    "        return responses\n",
    "\n",
    "# List of 5000 URLs to visit\n",
    "urls = unique_domains_no_summary\n",
    "\n",
    "# Run the scraper\n",
    "responses = asyncio.run(fetch_all(urls))\n",
    "\n",
    "# Convert results to a DataFrame\n",
    "df = pd.DataFrame(responses, columns=[\"Domain\", \"Text\"])\n",
    "\n",
    "# Evaluate the results\n",
    "successful_connections = df[df[\"Text\"].str.contains(\"Connection failed\") == False].shape[0]\n",
    "failed_connections = df[df[\"Text\"].str.contains(\"Connection failed\")].shape[0]\n",
    "\n",
    "print(f\"Successful connections: {successful_connections}\")\n",
    "print(f\"Failed connections: {failed_connections}\")\n",
    "\n",
    "# Display the first few rows of the dataframe\n",
    "print(df.head())\n",
    "\n",
    "# Save the results to a CSV file\n",
    "df.to_csv(\"scraped_data_round2.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "005d44f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "\n",
    "# Assuming response_summary_sample.choices[0].message.content is a JSON string\n",
    "json_content = response_summary_sample.choices[0].message.content\n",
    "\n",
    "# Parse the JSON string into a Python object (list of dictionaries)\n",
    "data = json.loads(json_content)\n",
    "\n",
    "# Store results in a CSV file\n",
    "with open(\"summary_results.csv\", \"w\", newline='') as file:\n",
    "    writer = csv.DictWriter(file, fieldnames=[\"domain\", \"summary_en\", \"summary_de\"])\n",
    "    writer.writeheader()\n",
    "    for entry in data['summaries']:\n",
    "        writer.writerow(entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53081192",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.3 Automatic link categorization with GPT-4o \n",
    "## Comparison of two approaches: just the domains, and the domains with the first 500 characters of text.\n",
    "\n",
    "## Preparation of the prompts\n",
    "\n",
    "intro_prompt = \"\"\"You are a categorizer of education web domains into predefined categories that only outputs in JSON. \n",
    "They are domains that are in the education ecosystem of Austria. The categories are linked to the roles they play in the innovation ecosystem.\n",
    "The categories you can choose from are:\"\"\"\n",
    "\n",
    "categories_prompt = \"\"\"{\n",
    "    \"National Ministry and Regulatory Bodies\": {\n",
    "        \"roles\": [\"Customers\"],\n",
    "        \"description\": \"Government entities responsible for overseeing and regulating education at the national level.\"\n",
    "    },\n",
    "    \"Local Authorities\": {\n",
    "        \"roles\": [\"Customers\", \"Suppliers\"],\n",
    "        \"description\": \"Regional or local government bodies that administer and support schools in their area.\"\n",
    "    },\n",
    "    \"Parent / Alumni Associations\": {\n",
    "        \"roles\": [\"Customers\"],\n",
    "        \"description\": \"Groups formed by parents and alumni to support school activities and represent community engagement.\"\n",
    "    },\n",
    "    \"Learning Management Systems and Administrative Tools\": {\n",
    "        \"roles\": [\"Suppliers\"],\n",
    "        \"description\": \"Software solutions that help schools manage and deliver educational content and administrative tasks.\"\n",
    "    },\n",
    "    \"Learning and Teaching Material Providers\": {\n",
    "        \"roles\": [\"Suppliers\"],\n",
    "        \"description\": \"Organizations that supply educational materials, including textbooks, digital content, and teaching aids.\"\n",
    "    },\n",
    "    \"Learning Experience Provider\": {\n",
    "        \"roles\": [\"Complementors\"],\n",
    "        \"description\": \"Entities offering supplementary educational experiences to students, such as workshops, courses, or extracurricular programs. Excludes businesses not related to schools.\"\n",
    "    },\n",
    "    \"(Social) Media Organizations and Platforms\": {\n",
    "        \"roles\": [\"Complementors\"],\n",
    "        \"description\": \"Platforms and organizations that facilitate communication and content sharing around educational communities.\"\n",
    "    },\n",
    "    \"Web Development Companies, Photography Services\": {\n",
    "        \"roles\": [\"Suppliers\"],\n",
    "        \"description\": \"Businesses providing digital and creative services like website development and photography. Excludes specialised educational services and platforms.\"\n",
    "    },\n",
    "    \"Psychological Counselling\": {\n",
    "        \"roles\": [\"Suppliers\"],\n",
    "        \"description\": \"(Psychologists, systemic therapists, psychotherapists and organizations offering mental health services and support to students and school staff.\"\n",
    "    },\n",
    "    \"Business Partners and Representation\": {\n",
    "        \"roles\": [\"Complementors\"],\n",
    "        \"description\": \"Companies and organizations not directly related to schools, excluding psychologists and psychotherapists.\"\n",
    "    },\n",
    "    \"Inter-School Organizations and International Networks\": {\n",
    "        \"roles\": [\"Complementors\"],\n",
    "        \"description\": \"Networks that connect schools for the purpose of collaboration, resource sharing, and international exchange.\"\n",
    "    },\n",
    "    \"Other Schools\": {\n",
    "        \"roles\": [\"Schools\"],\n",
    "        \"description\": \"Other official school institutions that might offer options for students to continue studying there.\"\n",
    "    },\n",
    "    \"Research, Academia, and Teacher Training\": {\n",
    "        \"roles\": [\"Suppliers\"],\n",
    "        \"description\": \"Institutions / universities involved in educational research, higher education, and the professional development of teachers.\"\n",
    "    }\n",
    "}\"\"\"\n",
    "\n",
    "intermediate_prompt_without_unsure = \"\"\"Exclusively choose from the above categories.\n",
    "To give orientation for your categorising: Customers pay for or request the educational services, Suppliers provide relevant inputs to the schools, and Complementors provide offers directly to the school and students.\n",
    "Do not choose Customers, Suppliers, Complementors or Other Roles as categories! They are there to help you select the right categories (key) from the list above.\n",
    "Do not create new categories.\"\"\"\n",
    "\n",
    "intermediate_prompt_with_unsure = \"\"\"Exclusively choose from the above categories. Don't guess, just respond with 'Unsure' if you do not have the information in your training data.\n",
    "To give orientation for your categorising: Customers pay for or request the educational services, Suppliers provide relevant inputs to the schools, and Complementors provide offers directly to the school and students.\n",
    "Do not choose Customers, Suppliers, Complementors or Other Roles as categories! They are there to help you select the right categories (key) from the list above.\n",
    "Do not create new categories.\"\"\"\n",
    "\n",
    "prompt_with_unsure = intro_prompt + categories_prompt + intermediate_prompt_with_unsure\n",
    "prompt_without_unsure = intro_prompt + categories_prompt + intermediate_prompt_without_unsure\n",
    "\n",
    "example_input_plain = \"\"\"\n",
    "klio.webuntis.com\n",
    "bundeskanzleramt.gv.at\n",
    "bfdi.bund.de\n",
    "newsletter.wko.at\n",
    "technikum-wien.at\n",
    "cba.media\n",
    "facebook.com\n",
    "frauenberatung.at\n",
    "humwien.at\n",
    "www1.schabi.ch\n",
    "bildung.bmbwf.gv.at\n",
    "ecoschools.global\n",
    "demokratiewebstatt.at\n",
    "de.wikipedia.org\n",
    "wiener-hak.at\n",
    "canva.com\n",
    "search.follettsoftware.com\n",
    "stem.org.uk\n",
    "kastner.at\n",
    "marianisten.at\n",
    "ms.bcfries.at\n",
    "bildung-wien.gv.at\n",
    "bmbwf.gv.at\n",
    "waisenversorgungsverein.org\n",
    "vip-uebungsfirma.jimdo.com\n",
    "jmtt.phwien.net\n",
    "voxmi.at\n",
    "card2brain.ch\n",
    "rabenhoftheater.com\n",
    "oe1.orf.at\n",
    "bmb.gv.at\n",
    "theaterimpark.at\n",
    "akademie-biberkor.de\n",
    "talentify.me\n",
    "lasalle.org\n",
    "bmbwf.gv.at\n",
    "kurier.at\n",
    "digitaleslernen.oead.at\n",
    "webmail.g19.at\n",
    "dsb.gv.at\n",
    "matura.gv.at\n",
    "internationalcampusvienna.at\n",
    "devmultisite.superfesch.at\n",
    "twyn.com\n",
    "steinedererinnerung.net\n",
    "facebook.com\n",
    "polgargym-my.sharepoint.com\n",
    "wohnpartner-wien.at\n",
    "youtube.com\n",
    "forms.office.com\"\"\"\n",
    "\n",
    "example_output_without_unsure = \"\"\"{ \"klio.webuntis.com\": \"Learning Management Systems and Administrative Tools\",\n",
    "  \"bundeskanzleramt.gv.at\": \"National Ministry and Regulatory Bodies\",\n",
    "  \"bfdi.bund.de\": \"National Ministry and Regulatory Bodies\",\n",
    "  \"newsletter.wko.at\": \"Business Partners and Representation\",\n",
    "  \"technikum-wien.at\": \"Research, Academia, and Teacher Training\",\n",
    "  \"cba.media\": \"(Social) Media Organizations and Platforms\",\n",
    "  \"facebook.com\": \"(Social) Media Organizations and Platforms\",\n",
    "  \"frauenberatung.at\": \"Psychological Counselling\",\n",
    "  \"humwien.at\": \"Inter-School Organizations and International Networks\",\n",
    "  \"www1.schabi.ch\": \"Learning and Teaching Material Providers\",\n",
    "  \"bildung.bmbwf.gv.at\": \"National Ministry and Regulatory Bodies\",\n",
    "  \"ecoschools.global\": \"Inter-School Organizations and International Networks\",\n",
    "  \"demokratiewebstatt.at\": \"Learning Experience Provider\",\n",
    "  \"de.wikipedia.org\": \"Learning Material Provider\",\n",
    "  \"wiener-hak.at\": \"Inter-School Organizations and International Networks\",\n",
    "  \"canva.com\": \"Web Development Companies, Photography Services\",\n",
    "  \"search.follettsoftware.com\": \"Learning and Teaching Material Providers\",\n",
    "  \"stem.org.uk\": \"Learning and Teaching Material Providers\",\n",
    "  \"kastner.at\": \"Business Partners and Representation\",\n",
    "  \"marianisten.at\": \"Other Schools\",\n",
    "  \"ms.bcfries.at\": \"Other Schools\",\n",
    "  \"bildung-wien.gv.at\": \"Local Authorities\",\n",
    "  \"bmbwf.gv.at\": \"National Ministry and Regulatory Bodies\",\n",
    "  \"waisenversorgungsverein.org\": \"Learning Experience Provider\",\n",
    "  \"vip-uebungsfirma.jimdo.com\": \"Web Development Companies, Photography Services\",\n",
    "  \"jmtt.phwien.net\": \"Research, Academia, and Teacher Training\",\n",
    "  \"voxmi.at\": \"Learning and Teaching Material Providers\",\n",
    "  \"card2brain.ch\": \"Learning and Teaching Material Providers\",\n",
    "  \"rabenhoftheater.com\": \"Learning Experience Provider\",\n",
    "  \"oe1.orf.at\": \"(Social) Media Organizations and Platforms\",\n",
    "  \"bmb.gv.at\": \"National Ministry and Regulatory Bodies\",\n",
    "  \"theaterimpark.at\": \"Learning Experience Provider\",\n",
    "  \"akademie-biberkor.de\": \"Research, Academia, and Teacher Training\",\n",
    "  \"talentify.me\": \"Learning Experience Provider\",\n",
    "  \"lasalle.org\": \"Other Schools\",\n",
    "  \"bmbwf.gv.at\": \"National Ministry and Regulatory Bodies\",\n",
    "  \"kurier.at\": \"(Social) Media Organizations and Platforms\",\n",
    "  \"digitaleslernen.oead.at\": \"Learning and Teaching Material Providers\",\n",
    "  \"webmail.g19.at\": \"Learning Management Systems and Administrative Tools\",\n",
    "  \"dsb.gv.at\": \"National Ministry and Regulatory Bodies\",\n",
    "  \"matura.gv.at\": \"National Ministry and Regulatory Bodies\",\n",
    "  \"internationalcampusvienna.at\": \"Inter-School Organizations and International Networks\",\n",
    "  \"devmultisite.superfesch.at\": \"Web Development Companies, Photography Services\",\n",
    "  \"twyn.com\": \"Learning Management Systems and Administrative Tools\",\n",
    "  \"steinedererinnerung.net\": \"Learning Experience Provider\",\n",
    "  \"facebook.com\": \"(Social) Media Organizations and Platforms\",\n",
    "  \"polgargym-my.sharepoint.com\": \"Learning Management Systems and Administrative Tools\",\n",
    "  \"wohnpartner-wien.at\": \"Local Authorities\",\n",
    "  \"youtube.com\": \"(Social) Media Organizations and Platforms\",\n",
    "  \"forms.office.com\": \"Learning Management Systems and Administrative Tools\"\n",
    "}\"\"\"\n",
    "example_output_with_unsure = \"\"\"{ \"klio.webuntis.com\": \"Learning Management Systems and Administrative Tools\",\n",
    "  \"bundeskanzleramt.gv.at\": \"National Ministry and Regulatory Bodies\",\n",
    "  \"bfdi.bund.de\": \"National Ministry and Regulatory Bodies\",\n",
    "  \"newsletter.wko.at\": \"Business Partners and Representation\",\n",
    "  \"technikum-wien.at\": \"Research, Academia, and Teacher Training\",\n",
    "  \"cba.media\": \"(Social) Media Organizations and Platforms\",\n",
    "  \"facebook.com\": \"(Social) Media Organizations and Platforms\",\n",
    "  \"frauenberatung.at\": \"Psychological Counselling\",\n",
    "  \"humwien.at\": \"Inter-School Organizations and International Networks\",\n",
    "  \"www1.schabi.ch\": \"Learning and Teaching Material Providers\",\n",
    "  \"bildung.bmbwf.gv.at\": \"National Ministry and Regulatory Bodies\",\n",
    "  \"ecoschools.global\": \"Inter-School Organizations and International Networks\",\n",
    "  \"demokratiewebstatt.at\": \"Learning Experience Provider\",\n",
    "  \"de.wikipedia.org\": \"Unsure\",\n",
    "  \"wiener-hak.at\": \"Inter-School Organizations and International Networks\",\n",
    "  \"canva.com\": \"Web Development Companies, Photography Services\",\n",
    "  \"search.follettsoftware.com\": \"Learning and Teaching Material Providers\",\n",
    "  \"stem.org.uk\": \"Learning and Teaching Material Providers\",\n",
    "  \"kastner.at\": \"Business Partners and Representation\",\n",
    "  \"marianisten.at\": \"Other Schools\",\n",
    "  \"ms.bcfries.at\": \"Other Schools\",\n",
    "  \"bildung-wien.gv.at\": \"Local Authorities\",\n",
    "  \"bmbwf.gv.at\": \"National Ministry and Regulatory Bodies\",\n",
    "  \"waisenversorgungsverein.org\": \"Unsure\",\n",
    "  \"vip-uebungsfirma.jimdo.com\": \"Web Development Companies, Photography Services\",\n",
    "  \"jmtt.phwien.net\": \"Research, Academia, and Teacher Training\",\n",
    "  \"voxmi.at\": \"Learning and Teaching Material Providers\",\n",
    "  \"card2brain.ch\": \"Learning and Teaching Material Providers\",\n",
    "  \"rabenhoftheater.com\": \"Learning Experience Provider\",\n",
    "  \"oe1.orf.at\": \"(Social) Media Organizations and Platforms\",\n",
    "  \"bmb.gv.at\": \"National Ministry and Regulatory Bodies\",\n",
    "  \"theaterimpark.at\": \"Learning Experience Provider\",\n",
    "  \"akademie-biberkor.de\": \"Research, Academia, and Teacher Training\",\n",
    "  \"talentify.me\": \"Learning Experience Provider\",\n",
    "  \"lasalle.org\": \"Other Schools\",\n",
    "  \"bmbwf.gv.at\": \"National Ministry and Regulatory Bodies\",\n",
    "  \"kurier.at\": \"(Social) Media Organizations and Platforms\",\n",
    "  \"digitaleslernen.oead.at\": \"Learning and Teaching Material Providers\",\n",
    "  \"webmail.g19.at\": \"Learning Management Systems and Administrative Tools\",\n",
    "  \"dsb.gv.at\": \"National Ministry and Regulatory Bodies\",\n",
    "  \"matura.gv.at\": \"National Ministry and Regulatory Bodies\",\n",
    "  \"internationalcampusvienna.at\": \"Inter-School Organizations and International Networks\",\n",
    "  \"devmultisite.superfesch.at\": \"Web Development Companies, Photography Services\",\n",
    "  \"twyn.com\": \"Learning Management Systems and Administrative Tools\",\n",
    "  \"steinedererinnerung.net\": \"Learning Experience Provider\",\n",
    "  \"facebook.com\": \"(Social) Media Organizations and Platforms\",\n",
    "  \"polgargym-my.sharepoint.com\": \"Learning Management Systems and Administrative Tools\",\n",
    "  \"wohnpartner-wien.at\": \"Local Authorities\",\n",
    "  \"youtube.com\": \"(Social) Media Organizations and Platforms\",\n",
    "  \"forms.office.com\": \"Learning Management Systems and Administrative Tools\"\n",
    "}\"\"\"\n",
    "\n",
    "example_input_webtext = \"\"\"\n",
    "{'stkarl.nikolausstiftung.at': 'St. Nikolausstiftung - Kindergarten St. Karl: Unser Haus Link zu: Kindergarten St. Karl: Unser HausUnser Haus Link zu: Kindergarten St. Karl: PädagogikPädagogik Link zu: Kindergarten St. Karl: FotosFotos Folge einem manuell hinzugefügten LinkKontakt ZurückWeiter123Herzlich willkommen im Pfarrkindergarten St. Karl Unsere Gruppe Team Informationen zur Anmeldung Informationen zu den Kosten Öffnungszeiten Wir freuen uns darauf, Sie und Ihr Kind kennenzulernen! Unser eingruppiger Kindergarten bietet eine sehr familiäre und persönliche Atmosphäre und ein Klima, das Freundlichkeit, Geborgenheit und Vertrautheit aufkommen lässt. In unserer Kindergartengruppe werden drei- bis sechsjährige Kinder betreut. Der besonders große Gruppenraum und der Turns'},\n",
    "{'medienmonster.info': 'Willkommen bei den MedienMonstern Projekte Leistungen Projekte für Kinder & Jugendliche Workshops für Fachkräfte Elternabende / Vorträge Materialien / Konzepte Schulentwicklung Angebotsbroschüre Partner Kooperationspartner Bildungsinstitutionen Förderer Über uns Team Machen Sie mit! Kontakt Kreative Medienbildung Medienprojekte, Fortbildungen & Materialien Zu den Angeboten Willkommen bei den MedienMonstern Die MedienMonster sind ein gemeinnütziger Verein, der mit starken Projekten den kreativen und sozialen Umgang mit neuen Medien bei Kindern und Jugendlichen fördert. 150 Bildungs-einrichtungen 9000 Kinder &Jugendliche 3500 PädagogischeFachkräfte 1250 Eltern Projekte Mit Games Geschichten erzählen An der Junior Uni Essen haben wir mit Jugen'},\n",
    "{'cbg-mittelhessen.de': '- CBG Mittelhessen CBG Mittelhessen Christliches Bildungszentrum für Gesundheitsberufe Menu + Das CBG + Die Schule Unser Team Jobs Die Ausbildung + Allgemeines Inhalte & Ziele Ausbildungsorte Was danach? Kontakt + Impressum Datenschutzerklärung Herzlich Willkommen auf den Seiten des Christlichen Bildungszentrums für Gesundheitsberufe (CBG Mittelhessen). Wir bilden Pflegefachfrauen und Pflegefachmänner (und divers) aus. Theorie und Praxis werden dabei optimal verknüpft: Während der dreijährigen Ausbildung findet der Theorieteil, im Wechsel mit den praktischen Einsätzen, in jeweils mehrwöchigen Unterrichtsblöcken am CBG statt. Im praktischen Teil durchlaufen die Auszubildenden die verschiedenen Stationen und Bereiche der jeweiligen Trägerkran'},\n",
    "{'ist.or.at': 'Institut für Systemische Therapie – Psychotherapie Beratung Supervision Coaching Skip to content KontaktLinksImpressumAGBsDatenschutzKlientInneninformation Institut für Systemische Therapie HomeVeranstaltungen AnmeldungHoffen, stärken, verändernCurriculum Systemische GruppentherapieSystemische Gruppentherapie. Einführung und RefresherAufmerksamkeitsfokussierung erzeugt UnterschiedeUnterschiedliches Wahrnehmen, Fühlen und Denken.Ein simultanes GruppentherapiekonzeptMethoden, die einen Unterschied machenWie viele Geschlechter kennt die Psychotherapie?Fürchte Dich nicht – so sehr!Trauma und Wirklichkeiten36 Stunden live dabei! Sommer-Intensiv-Training (SIT) 2024Unterschiede als RessourceLösungsorientierte Beratung nach systemischen GrundlagenÜ'}\"\"\"\n",
    "\n",
    "example_output_webtext = \"\"\"\n",
    "{'stkarl.nikolausstiftung.at': 'Other Schools',\n",
    "'medienmonster.info': 'Education Experience Provider',\n",
    "'cbg-mittelhessen.de': 'Other Schools'\n",
    "'ist.or.at': 'Psychological Counselling'}\"\"\"\n",
    "\n",
    "# Testing original web test vs. a summary\n",
    "\n",
    "example_input_summary = \"\"\"{'stkarl.nikolausstiftung.at': 'St. Nikolausstiftung operates a kindergarten focusing on pedagogical approaches and community involvement.'},\n",
    "{'medienmonster.info': 'MedienMonster offers media education projects and workshops for children and youth, focusing on creative media skills.'},\n",
    "{'cbg-mittelhessen.de': 'CBG Mittelhessen is a Christian educational center for health professions, offering training programs.'},\n",
    "{'ist.or.at': 'The Institute for Systemic Therapy offers psychotherapy, counseling, and training programs focusing on systemic approaches.'}\"\"\"\n",
    "\n",
    "example_output_summary = \"\"\"\n",
    "{'stkarl.nikolausstiftung.at': 'Category',\n",
    "'medienmonster.info': 'Category',\n",
    "'cbg-mittelhessen.de': 'Category'\n",
    "'ist.or.at': 'Category'}\"\"\"\n",
    "\n",
    "## Testing the prompts\n",
    "\n",
    "testset_input = \"\"\"appserei.com\n",
    "915053-redaktion.schulen.wien.gv.at\n",
    "paysafecard.com\n",
    "oeamtc.at\n",
    "baumgarten.nikolausstiftung.at\n",
    "ritakern.at\n",
    "music.amazon.de\n",
    "quiz.westermann.de\n",
    "lerntrick.com\n",
    "krcma.at\n",
    "jochenfallmann.at\n",
    "ifa.or.at\n",
    "meduniwien.ac.at\n",
    "ante-portas.at\n",
    "ista.ac.at\n",
    "praxis-widauer.at\n",
    "lasalle-assedil.org\n",
    "bildungszentrum-kenyongasse.at\n",
    "diepresse.com\n",
    "das.fotovonzinner.com\n",
    "bfi.at\n",
    "kija-wien.at\n",
    "kohaut.jimdofree.com\n",
    "efa.vor.at\n",
    "moodle.htlw10.at\n",
    "signon.springer.com\n",
    "moodle.lehrerweb.at\n",
    "bmj.gv.at\n",
    "lernmax.at\n",
    "merlin-technology.com\n",
    "stories.audible.com\n",
    "ita.or.at\n",
    "homepage.bildungsserver.com\n",
    "businessatschool.de\n",
    "iwik.at\n",
    "vwgh.gv.at\n",
    "praxis-psychotherapie-mediation.at\n",
    "waldorfschule-poetzleinsdorf.at\n",
    "kkunert.com\n",
    "orthografietrainer.net\n",
    "janegoodall.at\n",
    "chorusviennensis.at\n",
    "ulrikereisner.com\n",
    "kunterbuntewelt.at\n",
    "regionews.at\n",
    "dasjetzt.at\n",
    "juliascherbaum.at\n",
    "gesundheitskasse.at\n",
    "help.instagram.com\n",
    "openthesaurus.de\n",
    "popperabsolventen.com\n",
    "lehrstelleninfo.at\n",
    "digi4school.com\n",
    "download.moodle.org\n",
    "weckenmann.at\n",
    "hvimeo.com\n",
    "religion.orf.at\n",
    "wwwpolgargym.at\n",
    "gafa-absolventen.at\n",
    "seeseiten.buchkatalog.at\n",
    "goldenetramway.at\n",
    "oefeb2024.phwien.ac.at\n",
    "914121.schulen.wien.gv.at\n",
    "oebv.at\n",
    "umweltbildung.at\n",
    "esra.at\n",
    "apcoa.at\n",
    "news.microsoft.com\n",
    "app.classninjas.com\n",
    "steiner-hitech.at\n",
    "datenschutz-grundverordnung.eu\n",
    "wienzufuss.at\n",
    "wienersommerdeutschkurse.at\n",
    "conservatoriocimarosa.org\n",
    "weltweitunterrichten.at\n",
    "iro.ogu.edu.tr\n",
    "onb.ac.at\n",
    "geschichtsdidaktik.eu\n",
    "areeka.net\n",
    "bohrnpatricia.net\n",
    "co-paartherapie.at\n",
    "stkarl.nikolausstiftung.at\n",
    "medienmonster.info\n",
    "cbg-mittelhessen.de\n",
    "ist.or.at\n",
    "wegweiser-verlag.at\n",
    "langegger-dick.at\n",
    "humane-balance.at\n",
    "fachhochschulen.ac.at\n",
    "unicef.at\n",
    "christophschwarz.net\n",
    "hmc.org.uk\n",
    "psychotherapie-stpoelten.at\n",
    "privatakademie.de\n",
    "gabriella-walisch.at\n",
    "eeducation.at\n",
    "elternvereinwittelsbach.org\n",
    "tge-online.de\n",
    "fotoalbum.himmelhof-wien.at\n",
    "ifl.at\"\"\"\n",
    "\n",
    "testset_output = \"\"\"{\n",
    "    \"appserei.com\": \"Web Development Companies, Photography Services\",\n",
    "    \"915053-redaktion.schulen.wien.gv.at\": \"Local Authorities\",\n",
    "    \"paysafecard.com\": \"Business Partners and Representation\",\n",
    "    \"oeamtc.at\": \"Business Partners and Representation\",\n",
    "    \"baumgarten.nikolausstiftung.at\": \"Other Schools\",\n",
    "    \"ritakern.at\": \"Psychological Counselling\",\n",
    "    \"music.amazon.de\": \"(Social) Media Organizations and Platforms\",\n",
    "    \"quiz.westermann.de\": \"Learning and Teaching Material Providers\",\n",
    "    \"lerntrick.com\": \"Learning and Teaching Material Providers\",\n",
    "    \"krcma.at\": \"Psychological Counselling\",\n",
    "    \"jochenfallmann.at\": \"Psychological Counselling\",\n",
    "    \"ifa.or.at\": \"Inter-School Organizations and International Networks\",\n",
    "    \"meduniwien.ac.at\": \"Research, Academia, and Teacher Training\",\n",
    "    \"ante-portas.at\": \"Business Partners and Representation\",\n",
    "    \"ista.ac.at\": \"Research, Academia, and Teacher Training\",\n",
    "    \"praxis-widauer.at\": \"Psychological Counselling\",\n",
    "    \"lasalle-assedil.org\": \"Other Schools\",\n",
    "    \"bildungszentrum-kenyongasse.at\": \"Other Schools\",\n",
    "    \"diepresse.com\": \"(Social) Media Organizations and Platforms\",\n",
    "    \"das.fotovonzinner.com\": \"Web Development Companies, Photography Services\",\n",
    "    \"bfi.at\": \"Research, Academia, and Teacher Training\",\n",
    "    \"kija-wien.at\": \"Local Authorities\",\n",
    "    \"kohaut.jimdofree.com\": \"Learning and Teaching Material Providers\",\n",
    "    \"efa.vor.at\": \"Local Authorities\",\n",
    "    \"moodle.htlw10.at\": \"Learning Management Systems and Administrative Tools\",\n",
    "    \"signon.springer.com\": \"Learning and Teaching Material Providers\",\n",
    "    \"moodle.lehrerweb.at\": \"Learning Management Systems and Administrative Tools\",\n",
    "    \"bmj.gv.at\": \"National Ministry and Regulatory Bodies\",\n",
    "    \"lernmax.at\": \"Learning and Teaching Material Providers\",\n",
    "    \"merlin-technology.com\": \"Business Partners and Representation\",\n",
    "    \"stories.audible.com\": \"(Social) Media Organizations and Platforms\",\n",
    "    \"ita.or.at\": \"Research, Academia, and Teacher Training\",\n",
    "    \"homepage.bildungsserver.com\": \"Learning and Teaching Material Providers\",\n",
    "    \"businessatschool.de\": \"Learning Experience Provider\",\n",
    "    \"iwik.at\": \"Psychological Counselling\",\n",
    "    \"vwgh.gv.at\": \"National Ministry and Regulatory Bodies\",\n",
    "    \"praxis-psychotherapie-mediation.at\": \"Psychological Counselling\",\n",
    "    \"waldorfschule-poetzleinsdorf.at\": \"Other Schools\",\n",
    "    \"kkunert.com\": \"Psychological Counselling\",\n",
    "    \"orthografietrainer.net\": \"Learning and Teaching Material Providers\",\n",
    "    \"janegoodall.at\": \"Learning Experience Provider\",\n",
    "    \"chorusviennensis.at\": \"Learning Experience Provider\",\n",
    "    \"ulrikereisner.com\": \"Psychological Counselling\",\n",
    "    \"kunterbuntewelt.at\": \"Psychological Counselling\",\n",
    "    \"regionews.at\": \"(Social) Media Organizations and Platforms\",\n",
    "    \"dasjetzt.at\": \"Business Partners and Representation\",\n",
    "    \"juliascherbaum.at\": \"Psychological Counselling\",\n",
    "    \"gesundheitskasse.at\": \"Business Partners and Representation\",\n",
    "    \"help.instagram.com\": \"(Social) Media Organizations and Platforms\",\n",
    "    \"openthesaurus.de\": \"Learning and Teaching Material Providers\",\n",
    "    \"popperabsolventen.com\": \"Parent / Alumni Associations\",\n",
    "    \"lehrstelleninfo.at\": \"Learning and Teaching Material Providers\",\n",
    "    \"digi4school.com\": \"Learning and Teaching Material Providers\",\n",
    "    \"download.moodle.org\": \"Learning Management Systems and Administrative Tools\",\n",
    "    \"weckenmann.at\": \"Psychological Counselling\",\n",
    "    \"hvimeo.com\": \"(Social) Media Organizations and Platforms\",\n",
    "    \"religion.orf.at\": \"(Social) Media Organizations and Platforms\",\n",
    "    \"wwwpolgargym.at\": \"Other Schools\",\n",
    "    \"gafa-absolventen.at\": \"Parent / Alumni Associations\",\n",
    "    \"seeseiten.buchkatalog.at\": \"Learning and Teaching Material Providers\",\n",
    "    \"goldenetramway.at\": \"Learning Experience Provider\",\n",
    "    \"oefeb2024.phwien.ac.at\": \"Research, Academia, and Teacher Training\",\n",
    "    \"914121.schulen.wien.gv.at\": \"Local Authorities\",\n",
    "    \"oebv.at\": \"Learning and Teaching Material Providers\",\n",
    "    \"umweltbildung.at\": \"Learning and Teaching Material Providers\",\n",
    "    \"esra.at\": \"Psychological Counselling\",\n",
    "    \"apcoa.at\": \"Business Partners and Representation\",\n",
    "    \"news.microsoft.com\": \"(Social) Media Organizations and Platforms\",\n",
    "    \"app.classninjas.com\": \"Learning Management Systems and Administrative Tools\",\n",
    "    \"steiner-hitech.at\": \"Business Partners and Representation\",\n",
    "    \"datenschutz-grundverordnung.eu\": \"National Ministry and Regulatory Bodies\",\n",
    "    \"wienzufuss.at\": \"Learning Experience Provider\",\n",
    "    \"wienersommerdeutschkurse.at\": \"Learning Experience Provider\",\n",
    "    \"conservatoriocimarosa.org\": \"Other Schools\",\n",
    "    \"weltweitunterrichten.at\": \"Inter-School Organizations and International Networks\",\n",
    "    \"iro.ogu.edu.tr\": \"Research, Academia, and Teacher Training\",\n",
    "    \"onb.ac.at\": \"Research, Academia, and Teacher Training\",\n",
    "    \"geschichtsdidaktik.eu\": \"Research, Academia, and Teacher Training\",\n",
    "    \"areeka.net\": \"Learning and Teaching Material Providers\",\n",
    "    \"bohrnpatricia.net\": \"Psychological Counselling\",\n",
    "    \"co-paartherapie.at\": \"Psychological Counselling\",\n",
    "    \"stkarl.nikolausstiftung.at\": \"Other Schools\",\n",
    "    \"medienmonster.info\": \"Learning Experience Provider\",\n",
    "    \"cbg-mittelhessen.de\": \"Other Schools\",\n",
    "    \"ist.or.at\": \"Psychological Counselling\",\n",
    "    \"wegweiser-verlag.at\": \"Learning and Teaching Material Providers\",\n",
    "    \"langegger-dick.at\": \"Psychological Counselling\",\n",
    "    \"humane-balance.at\": \"Psychological Counselling\",\n",
    "    \"fachhochschulen.ac.at\": \"Research, Academia, and Teacher Training\",\n",
    "    \"unicef.at\": \"Learning Experience Provider\",\n",
    "    \"christophschwarz.net\": \"Web Development Companies, Photography Services\",\n",
    "    \"hmc.org.uk\": \"Inter-School Organizations and International Networks\",\n",
    "    \"psychotherapie-stpoelten.at\": \"Psychological Counselling\",\n",
    "    \"privatakademie.de\": \"Other Schools\",\n",
    "    \"gabriella-walisch.at\": \"Psychological Counselling\",\n",
    "    \"eeducation.at\": \"Learning and Teaching Material Providers\",\n",
    "    \"elternvereinwittelsbach.org\": \"Parent / Alumni Associations\",\n",
    "    \"tge-online.de\": \"Learning and Teaching Material Providers\",\n",
    "    \"fotoalbum.himmelhof-wien.at\": \"Web Development Companies, Photography Services\",\n",
    "    \"ifl.at\": \"Other Schools\"\n",
    "}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f5317a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.3.1 Categorization\n",
    "\n",
    "# open standardized_site_summaries.json\n",
    "import json\n",
    "\n",
    "# Open and read the JSON file\n",
    "with open('standardized_site_summaries.json', 'r') as file:\n",
    "    standardized_data = json.load(file)\n",
    "\n",
    "input_object = []\n",
    "for line in testset_input.split(\"\\n\"):\n",
    "    domain = line.strip()\n",
    "    if line in standardized_data.keys():\n",
    "        input_object.append({domain: standardized_data[domain][\"summary_en\"]})\n",
    "    else:\n",
    "        input_object.append({domain: \"\"})\n",
    "        \n",
    "\n",
    "response_4o = client.chat.completions.create(\n",
    "  model=\"gpt-4o\",\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": prompt_without_unsure},\n",
    "    {\"role\": \"user\", \"content\": example_input_summary},\n",
    "    {\"role\": \"assistant\", \"content\": example_output_summary},\n",
    "    {\"role\": \"user\", \"content\": testset_input}\n",
    "  ],\n",
    "    response_format={\n",
    "    \"type\": \"json_object\",  # Enforces JSON format\n",
    "  }, \n",
    "  temperature=0.0,\n",
    ")\n",
    "\n",
    "print(response_4o.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95baaefd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Ensure response_4o.choices[0].message.content is a valid JSON string\n",
    "response_content = response_4o.choices[0].message.content\n",
    "\n",
    "try:\n",
    "    # Parse the JSON string into a dictionary\n",
    "    response_json = json.loads(response_content)\n",
    "except json.JSONDecodeError as e:\n",
    "    print(f\"Error decoding JSON: {e}\")\n",
    "    response_json = {}\n",
    "\n",
    "# testset_output is a JSON string, so we need to parse it into a dictionary\n",
    "try:\n",
    "    testset_output_json = json.loads(testset_output)\n",
    "except json.JSONDecodeError as e:\n",
    "    print(f\"Error decoding JSON: {e}\")\n",
    "    testset_output_json = {}\n",
    "\n",
    "# Compare to testset_output_json and count correct answers\n",
    "correct = 0\n",
    "for domain, category in response_json.items():\n",
    "    if domain in testset_output_json:\n",
    "        if category == testset_output_json[domain]:\n",
    "            correct += 1\n",
    "\n",
    "print(f\"Correctly categorized domains: {correct} out of {len(testset_output_json)}\")\n",
    "\n",
    "# Print incorrect entries\n",
    "for domain, category in response_json.items():\n",
    "    if domain in testset_output_json:\n",
    "        if category != testset_output_json[domain]:\n",
    "            print(f\"Domain: {domain}, Predicted: {category}, Correct: {testset_output_json[domain]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d6c740",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preparation of all domains dictionary with domains and summaries where available\n",
    "domains_with_summaries = {}\n",
    "with open('standardized_site_summaries.json', 'r') as file:\n",
    "    standardized_data = json.load(file)\n",
    "# get links from unique domains\n",
    "# get summaries from standardized_site_summaries.json where available\n",
    "# for link in unique_domains:\n",
    "#     if link in standardized_data.keys():\n",
    "#         domains_with_summaries[link] = standardized_data[link][\"summary_en\"]\n",
    "#     else:\n",
    "#         domains_with_summaries[link] = \"\"\n",
    "\n",
    "\n",
    "# create dictionary with only domains that have summaries:\n",
    "for link in standardized_data.keys():\n",
    "    domains_with_summaries[link] = standardized_data[link][\"summary_en\"]\n",
    "\n",
    "# #tests\n",
    "print(len(domains_with_summaries))\n",
    "# # print 5 entries of domains_with_summaries\n",
    "for i, (domain, summary) in enumerate(domains_with_summaries.items()):\n",
    "    print(f\"{domain}: {summary}\")\n",
    "    if i == 4:\n",
    "        break\n",
    "\n",
    "\n",
    "# for a test run, we will use the first 100 domains\n",
    "#domains_with_summaries_sample = dict(list(domains_with_summaries.items())[:40])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac34f627",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Categorize those sites that don't have a category yet\n",
    "domains_without_category = {}\n",
    "\n",
    "# Open site_summaries_and_categories.json\n",
    "with open('site_summaries_and_categories.json', 'r') as file:\n",
    "    site_summaries_and_categories = json.load(file)\n",
    "\n",
    "for domain, details in site_summaries_and_categories.items():\n",
    "    # Check if 'category' key exists and if there's no category yet, add domain and summary_en to domains_without_category\n",
    "    if 'category' not in details:\n",
    "        domains_without_category[domain] = details['summary_en']\n",
    "\n",
    "# Print the domains without category\n",
    "print(domains_without_category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd4b3526",
   "metadata": {},
   "outputs": [],
   "source": [
    "# categorization in batches of 100\n",
    "import json\n",
    "import os\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "api_legacy_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "# Initialize OpenAI client\n",
    "client = OpenAI(api_key=api_legacy_key)\n",
    "\n",
    "# Function to process a batch of data\n",
    "def process_batch(batch):\n",
    "    try:\n",
    "        # Convert batch to JSON string\n",
    "        batch_str = json.dumps(batch)\n",
    "\n",
    "        # Make API request\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": prompt_without_unsure},\n",
    "                {\"role\": \"user\", \"content\": example_input_summary},\n",
    "                {\"role\": \"assistant\", \"content\": example_output_summary},\n",
    "                {\"role\": \"user\", \"content\": batch_str}\n",
    "            ],\n",
    "            response_format={\"type\": \"json_object\"},\n",
    "            temperature=0.1,\n",
    "        )\n",
    "\n",
    "        # Return the entire message content\n",
    "        return response.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing batch: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "# Function to divide dictionary into batches\n",
    "def divide_into_batches(data_dict, batch_size):\n",
    "    items = list(data_dict.items())\n",
    "    for i in range(0, len(items), batch_size):\n",
    "        yield dict(items[i:i + batch_size])\n",
    "\n",
    "# Main processing\n",
    "batch_size = 200\n",
    "df_dict_batches = divide_into_batches(domains_without_category, batch_size)\n",
    "\n",
    "# Initialize a text string to store results\n",
    "results_text = \"\"\n",
    "\n",
    "# Initialize a counter for batch number\n",
    "batch_number = 1\n",
    "# Process each batch and append results to the text string\n",
    "for batch in df_dict_batches:\n",
    "    print(f\"Processing batch {batch_number}\")\n",
    "    batch_result = process_batch(batch)\n",
    "    results_text += batch_result + \"\\n\"\n",
    "    batch_number += 1\n",
    "\n",
    "\n",
    "# Save the results to a text file\n",
    "with open(\"site_categorization_results_without_categories.txt\", \"w\") as file:\n",
    "    file.write(results_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02a36a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Read from site_categorization_results.txt\n",
    "with open('site_categorization_results_without_categories.txt', 'r') as file:\n",
    "    categorization_text = file.read()\n",
    "\n",
    "# Add the results to a dictionary\n",
    "results_dict = json.loads(categorization_text)\n",
    "\n",
    "# Load standardized_site_summaries.json\n",
    "with open('site_summaries_and_categories.json', 'r') as file:\n",
    "    sites_and_summaries = json.load(file)\n",
    "\n",
    "#print(sites_and_summaries[\"uszeged.hu\"])\n",
    "\n",
    "# Extend the standardized_site_summaries.json with the categories from the categorization\n",
    "for domain, category in results_dict.items():\n",
    "    if domain in sites_and_summaries:\n",
    "        # Only update the category if it doesn't already exist or is empty\n",
    "        if not sites_and_summaries[domain].get(\"category\"):\n",
    "            sites_and_summaries[domain][\"category\"] = category\n",
    "    else:\n",
    "        sites_and_summaries[domain] = {\"summary_en\": \"\", \"category\": category}\n",
    "\n",
    "# Save the extended dictionary to a new file, while keeping special characters readable instead of escaped\n",
    "with open(\"site_summaries_and_categories2.json\", \"w\") as file:\n",
    "    json.dump(sites_and_summaries, file, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58441c1c",
   "metadata": {},
   "source": [
    "# 6. Visualisation of the results in graph form, based on different filters \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18563f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample data\n",
    "\n",
    "# Load the site summaries and categories\n",
    "vis_sum_and_cat = {\"systempraxis.at\": {\n",
    "        \"summary_en\": \"SYSTEMPRAXIS.AT offers systemic psychotherapy and coaching services aimed at helping individuals navigate difficult times and find direction in life. They emphasize the importance of taking the first step towards personal growth and change.\",\n",
    "        \"summary_de\": \"SYSTEMPRAXIS.AT bietet systemische Psychotherapie und Coaching-Dienste an, die darauf abzielen, Menschen in schwierigen Zeiten zu helfen und Orientierung im Leben zu finden. Sie betonen die Bedeutung des ersten Schrittes in Richtung persönlicher Entwicklung und Veränderung.\",\n",
    "        \"category\": \"Psychological Counselling\"\n",
    "    },\n",
    "    \"abendgym-klagenfurt.at\": {\n",
    "        \"summary_en\": \"Abendgymnasium Klagenfurt is a public evening school offering free education and resources for students pursuing their Matura. They provide various study programs and support services for adult learners.\",\n",
    "        \"summary_de\": \"Das Abendgymnasium Klagenfurt ist eine öffentliche Abendschule, die kostenlose Bildung und Ressourcen für Schüler anbietet, die ihre Matura anstreben. Sie bieten verschiedene Studienprogramme und Unterstützungsdienste für Erwachsene an.\",\n",
    "        \"category\": \"Other Schools\"\n",
    "    },\n",
    "    \"cedefop.europa.eu\": {\n",
    "        \"summary_en\": \"CEDEFOP is the European Centre for the Development of Vocational Training, focusing on skills and labor market policies. They provide resources and support for vocational education and training for youth and adults.\",\n",
    "        \"summary_de\": \"CEDEFOP ist das Europäische Zentrum für die Entwicklung der Berufsbildung, das sich auf Fähigkeiten und Arbeitsmarktpolitik konzentriert. Sie bieten Ressourcen und Unterstützung für die berufliche Bildung und Ausbildung von Jugendlichen und Erwachsenen.\",\n",
    "        \"category\": \"Research, Academia, and Teacher Training\"\n",
    "    },\n",
    "    \"grundschulkoenig.de\": {\n",
    "        \"summary_en\": \"Grundschulkönig provides worksheets and teaching materials for elementary schools, offering a variety of free exercises in subjects like German, math, and general knowledge.\",\n",
    "        \"summary_de\": \"Grundschulkönig bietet Arbeitsblätter und Unterrichtsmaterialien für Grundschulen an und stellt eine Vielzahl von kostenlosen Übungen in Fächern wie Deutsch, Mathe und Sachkunde zur Verfügung.\",\n",
    "        \"category\": \"Learning and Teaching Material Providers\"\n",
    "    }}\n",
    "\n",
    "\n",
    "vis_sample_school_list = [{\n",
    "        \"SKZ\": \"910021\",\n",
    "        \"base_url\": \"https://alxinger.schule.wien.at\",\n",
    "        \"internal_links\": [\"https://alxinger.schule.wien.at\",\n",
    "            \"https://alxinger.schule.wien.at/something\",\n",
    "            \"https://alxinger.schule.wien.at/somethingelse\",\n",
    "            \"https://alxinger.schule.wien.at/somethingdifferent\",],\n",
    "        \"visited_internal_links\": [\n",
    "            \"https://alxinger.schule.wien.at/something\",\n",
    "            \"https://alxinger.schule.wien.at/somethingelse\"\n",
    "        ],\n",
    "        \"attempted_visits_links\": [\n",
    "            \"https://alxinger.schule.wien.at\",\n",
    "            \"https://alxinger.schule.wien.at/something\",\n",
    "            \"https://alxinger.schule.wien.at/somethingelse\"\n",
    "        ],\n",
    "        \"all_links\": [\"https://www.cedefop.europa.eu/en/about-cedefop/recruitment/traineeships\",\n",
    "                      \"https://www.grundschulkoenig.de/\",\n",
    "                      \"https://abendgym-klagenfurt.at\",\n",
    "                      \"http://www.systempraxis.at\",]\n",
    "        }]\n",
    "\n",
    "# VS Alxingergasse 82,1,Volksschule (öffentlich),http://alxinger.schule.wien.at/,\"10., Alxingergasse 82\",910021,Volksschule,öffentl.,https://alxinger.schule.wien.at/,False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c8c59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FILTERING\n",
    "import pandas as pd\n",
    "comparison_df = pd.read_csv('comparison_df_2024-08-12.csv')\n",
    "\n",
    "filtered_df = comparison_df[comparison_df[\"school_type_txt\"] == \"Allgemeinbildende höhere Schule (öffentlich)\"]\n",
    "# multiple schooltypes\n",
    "# filtered_df = comparison_df[comparison_df[\"school_type_txt\"].isin([\"Bundesgymnasium\", \"Bundesrealgymnasium\"])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3841df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Node-Edge Visualization Generator\n",
    "from pyvis.network import Network\n",
    "import networkx as nx\n",
    "\n",
    "\n",
    "def vis_ne_loader(input_df: pd.DataFrame, output_name: str, language: str):\n",
    "    summary_lang_string = \"summary_\" + language # en or de\n",
    "    # load input data\n",
    "    with open('scraped_links_2024-08-12_cleaned.json', 'r') as file:\n",
    "        scraped_links = json.load(file)\n",
    "    with open('site_summaries_and_categories.json', 'r') as file:\n",
    "        site_summaries_and_categories = json.load(file)\n",
    "    categories_dict = json.loads(categories_prompt)\n",
    "    # Create a new PyVis network\n",
    "    net = Network(height=\"100%\", width=\"100%\", bgcolor=\"#FFFFFF\", font_color=\"black\", notebook=True, cdn_resources='remote', select_menu=True)\n",
    "    \n",
    "    # Add nodes for each school\n",
    "    for index, row in input_df.iterrows():\n",
    "        # Find the matching dictionary\n",
    "        try:\n",
    "          matching_dict = next(d for d in scraped_links if d[\"SKZ\"] == str(row[\"SKZ\"]))\n",
    "        except StopIteration:\n",
    "          #print(f\"SKZ {row['SKZ']} not found in scraped_links\")\n",
    "          continue\n",
    "        \n",
    "        if matching_dict[\"count_int_links\"] == 0:\n",
    "          continue\n",
    "\n",
    "        # Print the base_url of the matching dictionary\n",
    "        #print(matching_dict[\"base_url\"])\n",
    "        \n",
    "        # Create the title\n",
    "        title = (\n",
    "            matching_dict[\"base_url\"] + \"\\n\" +\n",
    "            row[\"school_type_txt\"] + \"\\n\" +\n",
    "            row[\"address\"] + \"\\n\\nInternal Pages: \" +\n",
    "            str(matching_dict[\"count_int_links\"]) + \"\\n\\nExternal Domains:\\n\" +\n",
    "            \"\\n\".join(matching_dict[\"cleaned_ext_domains\"])\n",
    "        )\n",
    "        \n",
    "        # Add a node for the school\n",
    "        net.add_node(label=row[\"name\"], n_id=row[\"name\"], title=title, group=\"Schools\", shape=\"box\")\n",
    "        # Add a node for each external domain\n",
    "        for domain in matching_dict[\"cleaned_ext_domains\"]:\n",
    "            # Get the category from the site summaries and categories\n",
    "            try:\n",
    "              existing_node = net.get_node(domain)\n",
    "            except KeyError:\n",
    "              if domain in site_summaries_and_categories:\n",
    "                #print(domain)\n",
    "                category = site_summaries_and_categories[domain][\"category\"]\n",
    "                group = categories_dict[category][\"roles\"][0]\n",
    "                summary = site_summaries_and_categories[domain][summary_lang_string]\n",
    "                title = domain + \" (\" + category + \")\\n\\n\" + summary + \"\\n\\nPages linked to:\\n\"\n",
    "                for link in matching_dict[\"cleaned_ext_links\"]:\n",
    "                    if link.startswith(domain):\n",
    "                        title += link + \"\\n\"\n",
    "                net.add_node(n_id=domain, title=title, label=domain, group=group, shape=\"box\")\n",
    "                # add an edge between the school and the domain\n",
    "                net.add_edge(row[\"name\"], domain)\n",
    "\n",
    "              else:\n",
    "                  category = \"Category not found\"\n",
    "                  title = \"Couldn't reach the site \\n\\nPages linked to:\\n\"\n",
    "            else: \n",
    "              net.add_edge(row[\"name\"], domain)\n",
    "              for link in matching_dict[\"cleaned_ext_links\"]:\n",
    "                if link.startswith(domain): \n",
    "                  existing_node[\"title\"] += link + \"\\n\"\n",
    "            # Add the node\n",
    "\n",
    "            # Add an edge between the school and the domain\n",
    "            # net.add_edge(row[\"SKZ\"], domain)\n",
    "\n",
    "\n",
    "\n",
    "    # Set options for the visualization\n",
    "    options = \"\"\"\n",
    "    var options = {\n",
    "        \"groups\": {\n",
    "            \"Schools\": {\n",
    "                \"color\": {\n",
    "                    \"background\": \"#FA8029\",\n",
    "                    \"border\": \"#F26D00\"\n",
    "                },\n",
    "                \"font\": {\n",
    "                    \"color\": \"black\"\n",
    "                }\n",
    "            },\n",
    "            \"Suppliers\": {\n",
    "                \"color\": {\n",
    "                    \"background\": \"#A8CA8F\",\n",
    "                    \"border\": \"#9EC585\"\n",
    "                },\n",
    "                \"font\": {\n",
    "                    \"color\": \"black\"\n",
    "                }\n",
    "            },\n",
    "            \"Complementors\": {\n",
    "                \"color\": {\n",
    "                    \"background\": \"#FFBE00\",\n",
    "                    \"border\": \"#FFD770\"\n",
    "                },\n",
    "                \"font\": {\n",
    "                    \"color\": \"black\"\n",
    "                }\n",
    "            },\n",
    "            \"Customers\": {\n",
    "                \"color\": {\n",
    "                    \"background\": \"#4892D2\",\n",
    "                    \"border\": \"#4892D2\"\n",
    "                },\n",
    "                \"font\": {\n",
    "                    \"color\": \"black\"\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        \"physics\": {\n",
    "            \"enabled\": true,\n",
    "            \"solver\": \"forceAtlas2Based\",\n",
    "            \"forceAtlas2Based\": {\n",
    "                \"gravity\": -1000,\n",
    "                \"centralGravity\": 0.025,\n",
    "                \"springLength\": 150,\n",
    "                \"springStrength\": 0.08,\n",
    "                \"damping\": 0.9,\n",
    "                \"overlap\": 0\n",
    "            },\n",
    "            \"stabilization\": {\n",
    "                \"enabled\": true,\n",
    "                \"iterations\": 750,\n",
    "                \"updateInterval\": 25,\n",
    "                \"onlyDynamicEdges\": false,\n",
    "                \"fit\": true\n",
    "            },\n",
    "            \"minVelocity\": 0.1,\n",
    "            \"maxVelocity\": 10,\n",
    "            \"timestep\": 0.5,\n",
    "            \"adaptiveTimestep\": true\n",
    "        }\n",
    "    }\n",
    "    \"\"\"\n",
    "    #net.toggle_physics(False)\n",
    "    net.set_options(options)\n",
    "    net.prep_notebook()\n",
    "    # net.show_buttons(filter_=['physics'])\n",
    "\n",
    "    # Save the visualization to a file or display it in a Jupyter notebook\n",
    "    net.show(output_name)\n",
    "\n",
    "\n",
    "\n",
    "# vis_ne_loader(filtered_df, \"6ahs_OEFF.html\", \"en\")\n",
    "# vis_ne_loader(comparison_df[comparison_df[\"school_type_txt\"] == \"Statutschule (privat)\"], \"6statutschulen_en.html\", \"en\")\n",
    "# vis_ne_loader(comparison_df[comparison_df[\"school_type_txt\"] == \"Volksschule (öffentlich)\"], \"6volksschulen_OEFF.html\", \"en\")\n",
    "# vis_ne_loader(comparison_df[comparison_df[\"school_type_txt\"] == \"Volksschule (privat)\"], \"6volksschulen_PRIV.html\", \"en\")\n",
    "# create url-friendly outputs\n",
    "import re\n",
    "import unicodedata\n",
    "def slugify(value):\n",
    "    # Normalize the string to remove accents\n",
    "    value = unicodedata.normalize('NFKD', value).encode('ascii', 'ignore').decode('ascii')\n",
    "    # Convert to lowercase\n",
    "    value = value.lower()\n",
    "    # Remove parentheses and other special characters\n",
    "    value = re.sub(r'[^\\w\\s-]', '', value)\n",
    "    # Replace spaces and underscores with hyphens\n",
    "    value = re.sub(r'[-\\s]+', '-', value)\n",
    "    return value\n",
    "\n",
    "print(\"\\nNode - Edge school links in English and German:\")\n",
    "\n",
    "comparison_df_2024 = pd.read_csv('comparison_df_2024-08-12.csv')\n",
    "\n",
    "# Create files for schooltypes with private and public aggregated in ENGLISH\n",
    "for school_type in comparison_df_2024[\"school_type_mapping\"].unique():\n",
    "   output_filename = \"6_\" + slugify(school_type) + \"_NE_en.html\"\n",
    "   print(school_type + \" ENG\")\n",
    "   vis_ne_loader(comparison_df[comparison_df[\"school_type_mapping\"] == school_type], output_filename, \"en\")\n",
    "\n",
    "# Create files for schooltypes with separate schooltypes in ENGLISH\n",
    "for school_type in comparison_df_2024[\"school_type_txt\"].unique():\n",
    "   output_filename = \"6_\" + slugify(school_type) + \"_NE_en.html\"\n",
    "   print(school_type + \" ENG\")\n",
    "   vis_ne_loader(comparison_df[comparison_df[\"school_type_txt\"] == school_type], output_filename, \"en\")\n",
    "\n",
    "   \n",
    "# Create files for schooltypes with private and public aggregated in GERMAN\n",
    "for school_type in comparison_df_2024[\"school_type_mapping\"].unique():\n",
    "   output_filename = \"6_\" + slugify(school_type) + \"_NE_de.html\"\n",
    "   print(school_type + \" DE\")\n",
    "   vis_ne_loader(comparison_df[comparison_df[\"school_type_mapping\"] == school_type], output_filename, \"de\")\n",
    "\n",
    "# Create files for schooltypes with separate schooltypes in GERMAN\n",
    "for school_type in comparison_df_2024[\"school_type_txt\"].unique():\n",
    "   output_filename = \"6_\" + slugify(school_type) + \"_NE_de.html\"\n",
    "   print(school_type + \" DE\")\n",
    "   vis_ne_loader(comparison_df[comparison_df[\"school_type_txt\"] == school_type], output_filename, \"de\")\n",
    "\n",
    "\n",
    "\n",
    "# set output filename\n",
    "# set language?\n",
    "# get SKZ's from filtered_df\n",
    "# get basic quantitative & qualitative stats for each SKZ \n",
    "# get links / domains for each SKZ from scraped_links_2024-08-12_cleaned.json\n",
    "# get summary categories for each link domain from site_summaries_and_categories.json\n",
    "\n",
    "# links_visited = vis_sample_school_list[\"SKZ\"==\"911021\"][\"attempted_visits_links\"]\n",
    "# print(links_visited)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9afb4aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the unique school_type_mapping from comparison_df_2024-08-12.csv line by line\n",
    "comparison_df_2024 = pd.read_csv('comparison_df_2024-08-12.csv')\n",
    "\n",
    "for school_type in comparison_df_2024[\"school_type_mapping\"].unique():\n",
    "    print(school_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f8d21e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Aggregate Visualization Generator\n",
    "\n",
    "from pyvis.network import Network\n",
    "\n",
    "\n",
    "def vis_agg_loader(input_df: pd.DataFrame, output_name: str, language: str):\n",
    "    summary_lang_string = \"summary_\" + language # en or de\n",
    "    # load input data\n",
    "    with open('scraped_links_2024-08-12_cleaned.json', 'r') as file:\n",
    "        scraped_links = json.load(file)\n",
    "    with open('site_summaries_and_categories.json', 'r') as file:\n",
    "        site_summaries_and_categories = json.load(file)\n",
    "    categories_dict = json.loads(categories_prompt)\n",
    "    # Create a new PyVis network\n",
    "    net = Network(height=\"100%\", width=\"100%\", bgcolor=\"#FFFFFF\", font_color=\"black\", notebook=True, select_menu=True, cdn_resources='remote')\n",
    "    \n",
    "    # Create the category node for Schools\n",
    "    # TODO: create the right shape. \n",
    "    net.add_node(label=\"Schools\", n_id=\"Schools\", title=\"Schools\", group=\"Schools\", shape=\"box\", level=2, value=5, scaling={\"label\": {\"enabled\": True, \"min\": 5, \"max\": 50}})\n",
    "\n",
    "    # Add nodes for each school\n",
    "    for index, row in input_df.iterrows():\n",
    "        # Find the matching dictionary\n",
    "        try:\n",
    "          matching_dict = next(d for d in scraped_links if d[\"SKZ\"] == str(row[\"SKZ\"]))\n",
    "        except StopIteration:\n",
    "          #print(f\"SKZ {row['SKZ']} not found in scraped_links\")\n",
    "          continue\n",
    "        \n",
    "        if matching_dict[\"count_int_links\"] == 0:\n",
    "          continue\n",
    "\n",
    "        # Print the base_url of the matching dictionary\n",
    "        #print(matching_dict[\"base_url\"])\n",
    "        \n",
    "        # Create the title\n",
    "        title = (\n",
    "            matching_dict[\"base_url\"] + \"\\n\" +\n",
    "            row[\"school_type_txt\"] + \"\\n\" +\n",
    "            row[\"address\"] + \"\\n\\nInternal Pages: \" +\n",
    "            str(matching_dict[\"count_int_links\"]) + \"\\n\\nExternal Domains:\\n\" +\n",
    "            \"\\n\".join(matching_dict[\"cleaned_ext_domains\"])\n",
    "        )\n",
    "        \n",
    "        # Add a node for the school\n",
    "        net.add_node(label=row[\"name\"], n_id=matching_dict[\"base_url\"], title=title, group=\"Schools\", shape=\"box\", level=1)\n",
    "        # Add an edge to Schools node\n",
    "        net.add_edge(\"Schools\", matching_dict[\"base_url\"])\n",
    "        # Increase size of Schools node\n",
    "        #net.get_node(\"Schools\")[\"value\"] += 10\n",
    "        # Add a node for each external domain\n",
    "        for domain in matching_dict[\"cleaned_ext_domains\"]:\n",
    "            # Get the category from the site summaries and categories\n",
    "            try:\n",
    "              existing_node = net.get_node(domain)\n",
    "            except KeyError:\n",
    "              if domain in site_summaries_and_categories:\n",
    "                #print(domain)\n",
    "                category = site_summaries_and_categories[domain][\"category\"]\n",
    "                group = categories_dict[category][\"roles\"][0]\n",
    "                # Add category node if it doesn't exist\n",
    "                try:\n",
    "                  existing_node_cat = net.get_node(category)\n",
    "                except KeyError:\n",
    "                  net.add_node(label=category + \" (\" + group + \")\", n_id=category, title=\"Description:\" + categories_dict[category][\"description\"], group=group, shape=\"box\", level=2, value=5, scaling={\"label\": {\"enabled\": True, \"min\": 5, \"max\": 50}})\n",
    "                  net.add_edge(\"Schools\", category, value=10)\n",
    "                #else:\n",
    "                  #existing_node_cat[\"value\"] += 10\n",
    "                summary = site_summaries_and_categories[domain][summary_lang_string]\n",
    "                title = domain + \" (\" + category + \")\\n\\n\" + summary + \"\\n\\nPages linked to:\\n\"\n",
    "                for link in matching_dict[\"cleaned_ext_links\"]:\n",
    "                    if link.startswith(domain):\n",
    "                        title += link + \"\\n\"\n",
    "                net.add_node(n_id=domain, title=title, label=domain, group=group, shape=\"box\", level=1)\n",
    "                # add an edge between the school and the domain\n",
    "                net.add_edge(category, domain)\n",
    "\n",
    "              else:\n",
    "                  category = \"Category not found\"\n",
    "                  title = \"Couldn't reach the site \\n\\nPages linked to:\\n\"\n",
    "            else: \n",
    "            #   net.add_edge(row[\"SKZ\"], domain)\n",
    "              for link in matching_dict[\"cleaned_ext_links\"]:\n",
    "                if link.startswith(domain): \n",
    "                  existing_node[\"title\"] += link + \"\\n\"\n",
    "\n",
    "\n",
    "    # Set options for the visualization\n",
    "    options = \"\"\"\n",
    "    var options = {\n",
    "        \"groups\": {\n",
    "            \"Schools\": {\n",
    "                \"color\": {\n",
    "                    \"background\": \"#FA8029\",\n",
    "                    \"border\": \"#F26D00\"\n",
    "                },\n",
    "                \"font\": {\n",
    "                    \"color\": \"black\"\n",
    "                }\n",
    "            },\n",
    "            \"Suppliers\": {\n",
    "                \"color\": {\n",
    "                    \"background\": \"#A8CA8F\",\n",
    "                    \"border\": \"#9EC585\"\n",
    "                },\n",
    "                \"font\": {\n",
    "                    \"color\": \"black\"\n",
    "                }\n",
    "            },\n",
    "            \"Complementors\": {\n",
    "                \"color\": {\n",
    "                    \"background\": \"#FFBE00\",\n",
    "                    \"border\": \"#FFD770\"\n",
    "                },\n",
    "                \"font\": {\n",
    "                    \"color\": \"black\"\n",
    "                }\n",
    "            },\n",
    "            \"Customers\": {\n",
    "                \"color\": {\n",
    "                    \"background\": \"#4892D2\",\n",
    "                    \"border\": \"#4892D2\"\n",
    "                },\n",
    "                \"font\": {\n",
    "                    \"color\": \"black\"\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \"\"\"\n",
    "    net.set_options(options)\n",
    "    net.prep_notebook()\n",
    "    # net.show_buttons(filter_=['physics'])\n",
    "    #net.toggle_physics(False)\n",
    "\n",
    "    # Save the visualization to a file or display it in a Jupyter notebook\n",
    "    net.show(output_name)\n",
    "\n",
    "\n",
    "# vis_agg_loader(filtered_df, \"test.html\", \"en\")\n",
    "\n",
    "# create url-friendly outputs\n",
    "import re\n",
    "import unicodedata\n",
    "def slugify(value):\n",
    "    # Normalize the string to remove accents\n",
    "    value = unicodedata.normalize('NFKD', value).encode('ascii', 'ignore').decode('ascii')\n",
    "    # Convert to lowercase\n",
    "    value = value.lower()\n",
    "    # Remove parentheses and other special characters\n",
    "    value = re.sub(r'[^\\w\\s-]', '', value)\n",
    "    # Replace spaces and underscores with hyphens\n",
    "    value = re.sub(r'[-\\s]+', '-', value)\n",
    "    return value\n",
    "\n",
    "comparison_df_2024 = pd.read_csv('comparison_df_2024-08-12.csv')\n",
    "\n",
    "print(\"\\nAggregated school links in English and German:\")\n",
    "\n",
    "# Create files for schooltypes with private and public aggregated in ENGLISH\n",
    "for school_type in comparison_df_2024[\"school_type_mapping\"].unique():\n",
    "   output_filename = \"6_\" + slugify(school_type) + \"_AGG_en.html\"\n",
    "   print(school_type + \" ENG\")\n",
    "   vis_agg_loader(comparison_df[comparison_df[\"school_type_mapping\"] == school_type], output_filename, \"en\")\n",
    "\n",
    "# Create files for schooltypes with separate schooltypes in ENGLISH\n",
    "for school_type in comparison_df_2024[\"school_type_txt\"].unique():\n",
    "   output_filename = \"6_\" + slugify(school_type) + \"_AGG_en.html\"\n",
    "   print(school_type + \" ENG\")\n",
    "   vis_agg_loader(comparison_df[comparison_df[\"school_type_txt\"] == school_type], output_filename, \"en\")\n",
    "\n",
    "   \n",
    "# Create files for schooltypes with private and public aggregated in GERMAN\n",
    "for school_type in comparison_df_2024[\"school_type_mapping\"].unique():\n",
    "   output_filename = \"6_\" + slugify(school_type) + \"_AGG_de.html\"\n",
    "   print(school_type + \" DE\")\n",
    "   vis_agg_loader(comparison_df[comparison_df[\"school_type_mapping\"] == school_type], output_filename, \"de\")\n",
    "\n",
    "# Create files for schooltypes with separate schooltypes in GERMAN\n",
    "for school_type in comparison_df_2024[\"school_type_txt\"].unique():\n",
    "   output_filename = \"6_\" + slugify(school_type) + \"_AGG_de.html\"\n",
    "   print(school_type + \" DE\")\n",
    "   vis_agg_loader(comparison_df[comparison_df[\"school_type_txt\"] == school_type], output_filename, \"de\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "928250c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the html file to work on Chrome too\n",
    "import os\n",
    "\n",
    "# Directory containing the HTML files\n",
    "directory = \".\"\n",
    "\n",
    "# The string to find and the replacement string\n",
    "old_line = '<div class=\"card\" style=\"width: 100%\">'\n",
    "new_line = '<div class=\"card\" style=\"width: 100%; height: 100%\">'\n",
    "\n",
    "# Loop through all files in the directory\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.startswith(\"6_\") and filename.endswith(\".html\"):\n",
    "        filepath = os.path.join(directory, filename)\n",
    "        \n",
    "        # Read the contents of the file\n",
    "        with open(filepath, \"r\") as file:\n",
    "            content = file.read()\n",
    "        \n",
    "        # Replace the old line with the new line\n",
    "        new_content = content.replace(old_line, new_line)\n",
    "        \n",
    "        # Write the modified content back to the file\n",
    "        with open(filepath, \"w\") as file:\n",
    "            file.write(new_content)\n",
    "\n",
    "print(\"Replacement completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc402637",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample visualisation to check if sizes work with the box\n",
    "# Create a new PyVis network\n",
    "net = Network(height=\"100%\", width=\"100%\", bgcolor=\"#FFFFFF\", font_color=\"black\", notebook=True, cdn_resources='remote', select_menu=True)\n",
    "net.add_node(label=\"Schools\", n_id=\"Schools\", title=\"Schools\", group=\"Schools\", shape=\"box\")\n",
    "net.add_node(label=\"Category\", n_id=\"Category\", title=\"Category\", group=\"Category\", shape=\"box\", value=10, scaling={\"label\": {\"enabled\": True, \"min\": 5, \"max\": 50}})\n",
    "# net.add_node(label=\"Category\", n_id=\"Category2\", title=\"Category2\", group=\"Category\", shape=\"box\", value=4, scaling={\"min\":0, \"max\":10, \"label\": {\"enabled\": True, \"min\": 5, \"max\": 50}, \"maxVisible\": 40})\n",
    "net.add_node(label=\"Category\", n_id=\"Category2\", title=\"Category2\", group=\"Category\", shape=\"box\", value=10, scaling={\"min\":0, \"max\":10, \"label\": {\"enabled\": True, \"min\": 5, \"max\": 50}})\n",
    "\n",
    "net.add_node(label=\"School\", n_id=\"School\", title=\"School\", group=\"School\", shape=\"custom\", )\n",
    "net.add_edge(\"Schools\", \"Category\")\n",
    "net.show(\"test2.html\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2839f55d",
   "metadata": {},
   "source": [
    "# 7 Categorization Statistics\n",
    "How many schools link to which categories\n",
    "Exclude those schools with 0 internal links -> Start with links. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b04bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# percentages of schools having at least one type of a link\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Load the schools dataset from the CSV file\n",
    "comparison_df = pd.read_csv('comparison_df_2024-08-12.csv')\n",
    "\n",
    "# Load the scraped links data from the JSON file\n",
    "with open('scraped_links_2024-08-12_cleaned.json') as f:\n",
    "    scraped_links = json.load(f)\n",
    "\n",
    "# Convert scraped_links to DataFrame for easier processing\n",
    "scraped_links_df = pd.DataFrame(scraped_links)\n",
    "\n",
    "# Filter out schools with 'count_int_links' == 0\n",
    "filtered_scraped_links_df = scraped_links_df[scraped_links_df['count_int_links'] > 0]\n",
    "\n",
    "# Merge the filtered scraped links data with the schools dataset\n",
    "merged_df = comparison_df.merge(filtered_scraped_links_df, on='SKZ', how='inner')\n",
    "\n",
    "# Load the site summaries and categories\n",
    "with open('site_summaries_and_categories.json') as f:\n",
    "    site_summaries_and_categories = json.load(f)\n",
    "\n",
    "# Map each domain to its category\n",
    "domain_to_category = {domain: details['category'] for domain, details in site_summaries_and_categories.items()}\n",
    "\n",
    "# Function to categorize external domains\n",
    "def categorize_domains(domain_list, domain_to_category):\n",
    "    categories = {category: 0 for category in set(domain_to_category.values())}\n",
    "    for domain in domain_list:\n",
    "        if domain in domain_to_category:\n",
    "            category = domain_to_category[domain]\n",
    "            categories[category] = 1\n",
    "    return categories\n",
    "\n",
    "# Apply the categorize_domains function to each school's external domains\n",
    "merged_df['domain_categories'] = merged_df['cleaned_ext_domains'].apply(categorize_domains, args=(domain_to_category,))\n",
    "\n",
    "# Convert the domain categories into separate columns\n",
    "domain_categories_df = pd.json_normalize(merged_df['domain_categories'])\n",
    "\n",
    "# Concatenate the original DataFrame with the domain categories DataFrame\n",
    "result_df = pd.concat([merged_df[['school_type_txt']], domain_categories_df], axis=1)\n",
    "\n",
    "# Calculate the percentage of each school type having links from each category\n",
    "percentage_df = result_df.groupby('school_type_txt').mean() * 100\n",
    "\n",
    "# Calculate the number of observations (n) for each school type\n",
    "n_values = result_df.groupby('school_type_txt').size()\n",
    "\n",
    "# Combine the number of observations with the percentage DataFrame\n",
    "percentage_df['n'] = n_values\n",
    "\n",
    "# Calculate the sum of n for the \"Total\" row and the mean of all other columns\n",
    "total_row = percentage_df.mean(numeric_only=True).to_frame().T\n",
    "total_row['n'] = n_values.sum()  # Use the sum for the 'n' column\n",
    "total_row.index = ['Total']\n",
    "\n",
    "# Concatenate the \"Total\" row with the percentage DataFrame\n",
    "percentage_df = pd.concat([percentage_df, total_row])\n",
    "\n",
    "# Generating LaTeX table code with overlapping diagonal headers and an additional \"n\" column\n",
    "latex_code = \"\\\\begin{adjustbox}{width=\\\\textwidth}\\n\\\\begin{tabular}{l|c|\" + \"c|\" * len(percentage_df.columns) + \"}\\n\\\\toprule\\n\"\n",
    "\n",
    "# Create overlapping diagonal column headers using \\makebox with invisible borders\n",
    "latex_code += \"School Type & n & \"\n",
    "for col in percentage_df.columns[:-1]:  # Exclude the \"n\" column from diagonal headers\n",
    "    latex_code += f\"\\\\makebox[2.2cm][l]{{\\\\rotatebox{{40}}{{{col}}}}} & \"\n",
    "latex_code = latex_code.rstrip(\"& \") + \" \\\\\\\\\\n\\\\midrule\\n\"\n",
    "\n",
    "# Add data rows\n",
    "for index, row in percentage_df.iterrows():\n",
    "    latex_code += f\"{index} & {int(row['n'])} & \" + \" & \".join(f\"{val:.2f}\\\\%\" for val in row[:-1]) + \" \\\\\\\\\\n\"\n",
    "\n",
    "# Add a final \"Total\" row with average values\n",
    "total_row = percentage_df.mean().to_frame().T\n",
    "total_row.index = ['Total']\n",
    "latex_code += \"\\\\midrule\\n\"\n",
    "latex_code += f\"Total & {int(total_row['n'].sum())} & \" + \" & \".join(f\"{val:.2f}\\\\%\" for val in total_row.iloc[0, :-1]) + \" \\\\\\\\\\n\"\n",
    "\n",
    "latex_code += \"\\\\bottomrule\\n\\\\end{tabular}\\n\\\\end{adjustbox}\"\n",
    "\n",
    "# Display the generated LaTeX code\n",
    "print(latex_code)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ed1bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unique domains \n",
    "import pandas as pd\n",
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "# Load the data\n",
    "comparison_df = pd.read_csv('comparison_df_2024-08-12.csv')\n",
    "with open('scraped_links_2024-08-12_cleaned.json', 'r') as file:\n",
    "    scraped_links = json.load(file)\n",
    "with open('site_summaries_and_categories.json', 'r') as file:\n",
    "    site_summaries_and_categories = json.load(file)\n",
    "\n",
    "# Initialize a dictionary to store the links\n",
    "category_links = defaultdict(lambda: defaultdict(list))\n",
    "\n",
    "# Process each school in the comparison_df\n",
    "for _, row in comparison_df.iterrows():\n",
    "    school_category = row['school_type_mapping']\n",
    "    skz = str(row['SKZ'])\n",
    "    \n",
    "    # Find the matching dictionary in scraped_links\n",
    "    matching_dict = next((d for d in scraped_links if d[\"SKZ\"] == skz), None)\n",
    "    if not matching_dict:\n",
    "        continue\n",
    "    \n",
    "    # Get the unique domains linked to\n",
    "    unique_domains = set(matching_dict[\"cleaned_ext_domains\"])\n",
    "    \n",
    "    # Collect the links in lists\n",
    "    for domain in unique_domains:\n",
    "        if domain in site_summaries_and_categories:\n",
    "            category = site_summaries_and_categories[domain][\"category\"]\n",
    "            category_links[school_category][category].append(domain)\n",
    "\n",
    "# Convert lists to sets to ensure uniqueness and count the unique links\n",
    "category_counts = defaultdict(lambda: defaultdict(int))\n",
    "total_unique_domains = defaultdict(set)\n",
    "\n",
    "for school_category, categories in category_links.items():\n",
    "    for category, links in categories.items():\n",
    "        unique_links = set(links)\n",
    "        category_counts[school_category][category] = len(unique_links)\n",
    "        total_unique_domains[category].update(unique_links)\n",
    "\n",
    "# Create a DataFrame from the category counts\n",
    "category_counts_df = pd.DataFrame(category_counts).fillna(0).astype(int)\n",
    "\n",
    "# Transpose the DataFrame to switch rows and columns\n",
    "category_counts_df = category_counts_df.T\n",
    "\n",
    "# Calculate the total unique domains for each category\n",
    "total_counts = {category: len(domains) for category, domains in total_unique_domains.items()}\n",
    "total_counts_df = pd.DataFrame(total_counts, index=['Total (Unique)'])\n",
    "\n",
    "# Concatenate the total counts row to the DataFrame\n",
    "category_counts_df = pd.concat([total_counts_df, category_counts_df])\n",
    "\n",
    "# Transpose the DataFrame to switch rows and columns\n",
    "category_counts_df = category_counts_df.T\n",
    "\n",
    "\n",
    "# Generate the LaTeX table without borders\n",
    "latex_table = category_counts_df.to_latex(index=True, header=True, escape=False, column_format='c' * (len(category_counts_df.columns) + 1))\n",
    "\n",
    "# Remove the \\toprule, \\midrule, and \\bottomrule commands to eliminate borders\n",
    "latex_table = latex_table.replace('\\\\toprule', '')\n",
    "latex_table = latex_table.replace('\\\\midrule', '')\n",
    "latex_table = latex_table.replace('\\\\bottomrule', '')\n",
    "latex_table = latex_table.replace('\\\\hline', '')\n",
    "\n",
    "# Remove commas from numbers\n",
    "latex_table = latex_table.replace(',', '')\n",
    "\n",
    "# Wrap the table with \\resizebox to fit the page width\n",
    "latex_table = f\"\\\\resizebox{{\\\\textwidth}}{{!}}{{{latex_table}}}\"\n",
    "\n",
    "# Print the LaTeX table\n",
    "print(latex_table)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "schoolscraperenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
